Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=5207.4568, lr=2.695e-05
step 50: loss=6374.2764, lr=5.190e-05
step 75: loss=6580.6804, lr=7.685e-05
train_loss=6736.6841  val_loss=1746.5918
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 2/50
step 109: loss=1813.2324, lr=1.108e-04
step 134: loss=1513.6151, lr=1.357e-04
step 159: loss=1536.7137, lr=1.607e-04
train_loss=1488.3646  val_loss=710.2868
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 3/50
step 193: loss=894.3358, lr=1.946e-04
step 218: loss=933.0143, lr=2.196e-04
step 243: loss=903.4450, lr=2.445e-04
train_loss=876.2796  val_loss=396.9347
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 4/50
step 277: loss=772.4269, lr=2.784e-04
step 302: loss=810.1494, lr=3.034e-04
step 327: loss=978.9584, lr=3.283e-04
train_loss=927.1726  val_loss=369.2126
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 5/50
step 361: loss=515.7671, lr=3.623e-04
step 386: loss=628.7523, lr=3.872e-04
step 411: loss=637.4822, lr=4.122e-04
train_loss=628.3515  val_loss=460.0999

Epoch 6/50
step 445: loss=616.9695, lr=4.461e-04
step 470: loss=687.8250, lr=4.711e-04
step 495: loss=613.4278, lr=4.960e-04
train_loss=584.9478  val_loss=550.7836

Epoch 7/50
step 529: loss=485.1764, lr=4.999e-04
step 554: loss=517.6747, lr=4.997e-04
step 579: loss=523.1915, lr=4.994e-04
train_loss=508.4675  val_loss=222.0826
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 8/50
step 613: loss=505.9485, lr=4.989e-04
step 638: loss=414.1524, lr=4.983e-04
step 663: loss=408.8181, lr=4.976e-04
train_loss=397.4341  val_loss=262.8932

Epoch 9/50
step 697: loss=274.6382, lr=4.965e-04
step 722: loss=403.0225, lr=4.956e-04
step 747: loss=370.0787, lr=4.945e-04
train_loss=359.2380  val_loss=205.4083
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 10/50
step 781: loss=973.2705, lr=4.929e-04
step 806: loss=611.8496, lr=4.916e-04
step 831: loss=484.0784, lr=4.902e-04
train_loss=457.6035  val_loss=150.8074
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 11/50
step 865: loss=175.4481, lr=4.881e-04
step 890: loss=189.6004, lr=4.864e-04
step 915: loss=199.9331, lr=4.847e-04
train_loss=198.6934  val_loss=207.1255

Epoch 12/50
step 949: loss=240.7580, lr=4.821e-04
step 974: loss=256.9508, lr=4.801e-04
step 999: loss=228.2097, lr=4.779e-04
train_loss=219.3510  val_loss=109.6156
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 13/50
step 1033: loss=158.9630, lr=4.749e-04
step 1058: loss=168.2108, lr=4.725e-04
step 1083: loss=162.0823, lr=4.701e-04
train_loss=160.8385  val_loss=111.0592

Epoch 14/50
step 1117: loss=139.7797, lr=4.665e-04
step 1142: loss=133.5833, lr=4.638e-04
step 1167: loss=147.6331, lr=4.610e-04
train_loss=147.3356  val_loss=99.2843
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 15/50
step 1201: loss=151.8124, lr=4.571e-04
step 1226: loss=147.9800, lr=4.541e-04
step 1251: loss=160.5617, lr=4.510e-04
train_loss=161.1315  val_loss=120.0973

Epoch 16/50
step 1285: loss=178.9580, lr=4.466e-04
step 1310: loss=176.2411, lr=4.433e-04
step 1335: loss=173.0888, lr=4.399e-04
train_loss=168.1521  val_loss=105.2752

Epoch 17/50
step 1369: loss=137.2596, lr=4.351e-04
step 1394: loss=133.1265, lr=4.315e-04
step 1419: loss=141.2733, lr=4.278e-04
train_loss=139.4726  val_loss=81.5638
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 18/50
step 1453: loss=119.4985, lr=4.227e-04
step 1478: loss=127.4220, lr=4.188e-04
step 1503: loss=131.9006, lr=4.149e-04
train_loss=131.7788  val_loss=259.5835

Epoch 19/50
step 1537: loss=147.9851, lr=4.094e-04
step 1562: loss=145.0078, lr=4.053e-04
step 1587: loss=144.7618, lr=4.011e-04
train_loss=139.1578  val_loss=78.6125
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 20/50
step 1621: loss=121.3582, lr=3.953e-04
step 1646: loss=538.0666, lr=3.909e-04
step 1671: loss=396.7804, lr=3.865e-04
train_loss=366.3427  val_loss=68.0375
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 21/50
step 1705: loss=131.9601, lr=3.804e-04
step 1730: loss=117.8598, lr=3.759e-04
step 1755: loss=107.9337, lr=3.713e-04
train_loss=111.0382  val_loss=66.0104
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 22/50
step 1789: loss=85.4984, lr=3.649e-04
step 1814: loss=112.6161, lr=3.602e-04
step 1839: loss=119.2906, lr=3.554e-04
train_loss=158.0622  val_loss=118.5687

Epoch 23/50
step 1873: loss=114.2419, lr=3.488e-04
step 1898: loss=125.5316, lr=3.439e-04
step 1923: loss=125.8883, lr=3.390e-04
train_loss=125.8931  val_loss=98.1605

Epoch 24/50
step 1957: loss=123.5230, lr=3.322e-04
step 1982: loss=113.7727, lr=3.272e-04
step 2007: loss=119.3273, lr=3.221e-04
train_loss=120.9458  val_loss=67.2362

Epoch 25/50
step 2041: loss=104.1019, lr=3.152e-04
step 2066: loss=118.2306, lr=3.101e-04
step 2091: loss=115.7285, lr=3.049e-04
train_loss=112.9974  val_loss=267.3684

Epoch 26/50
step 2125: loss=99.4822, lr=2.979e-04
step 2150: loss=93.4437, lr=2.927e-04
step 2175: loss=95.2177, lr=2.874e-04
train_loss=93.4320  val_loss=59.2813
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 27/50
step 2209: loss=78.5576, lr=2.803e-04
step 2234: loss=80.2913, lr=2.750e-04
step 2259: loss=84.5690, lr=2.698e-04
train_loss=84.6264  val_loss=97.6513

Epoch 28/50
step 2293: loss=100.8425, lr=2.626e-04
step 2318: loss=104.5760, lr=2.573e-04
step 2343: loss=103.3365, lr=2.520e-04
train_loss=102.4264  val_loss=73.5659

Epoch 29/50
step 2377: loss=124.8449, lr=2.448e-04
step 2402: loss=123.2053, lr=2.395e-04
step 2427: loss=111.4030, lr=2.342e-04
train_loss=107.9254  val_loss=66.3818

Epoch 30/50
step 2461: loss=101.7795, lr=2.270e-04
step 2486: loss=117.1136, lr=2.218e-04
step 2511: loss=104.7351, lr=2.165e-04
train_loss=101.6724  val_loss=69.8737

Epoch 31/50
step 2545: loss=73.3756, lr=2.094e-04
step 2570: loss=90.5360, lr=2.042e-04
step 2595: loss=81.6118, lr=1.990e-04
train_loss=80.4200  val_loss=50.4328
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 32/50
step 2629: loss=63.7440, lr=1.919e-04
step 2654: loss=69.6191, lr=1.868e-04
step 2679: loss=78.7723, lr=1.817e-04
train_loss=77.5585  val_loss=46.3463
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 33/50
step 2713: loss=67.7737, lr=1.748e-04
step 2738: loss=67.5857, lr=1.698e-04
step 2763: loss=67.3162, lr=1.648e-04
train_loss=69.5432  val_loss=50.6535

Epoch 34/50
step 2797: loss=70.0254, lr=1.581e-04
step 2822: loss=81.2962, lr=1.532e-04
step 2847: loss=77.3523, lr=1.483e-04
train_loss=75.5345  val_loss=84.0371

Epoch 35/50
step 2881: loss=60.9827, lr=1.418e-04
step 2906: loss=67.8160, lr=1.370e-04
step 2931: loss=69.2384, lr=1.323e-04
train_loss=70.2652  val_loss=44.4511
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 36/50
step 2965: loss=57.2199, lr=1.261e-04
step 2990: loss=61.0618, lr=1.215e-04
step 3015: loss=66.2460, lr=1.170e-04
train_loss=66.2035  val_loss=49.8006

Epoch 37/50
step 3049: loss=124.7485, lr=1.110e-04
step 3074: loss=89.7786, lr=1.066e-04
step 3099: loss=84.5476, lr=1.023e-04
train_loss=82.3742  val_loss=51.5945

Epoch 38/50
step 3133: loss=82.0495, lr=9.658e-05
step 3158: loss=72.0161, lr=9.244e-05
step 3183: loss=79.0397, lr=8.838e-05
train_loss=76.4822  val_loss=34.9586
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 39/50
step 3217: loss=69.3031, lr=8.298e-05
step 3242: loss=63.3016, lr=7.909e-05
step 3267: loss=60.5693, lr=7.528e-05
train_loss=63.0651  val_loss=38.5952

Epoch 40/50
step 3301: loss=55.5723, lr=7.023e-05
step 3326: loss=62.8135, lr=6.660e-05
step 3351: loss=60.4466, lr=6.307e-05
train_loss=60.0287  val_loss=43.3303

Epoch 41/50
step 3385: loss=63.0870, lr=5.839e-05
step 3410: loss=69.5068, lr=5.506e-05
step 3435: loss=65.0447, lr=5.181e-05
train_loss=64.4482  val_loss=35.9598

Epoch 42/50
step 3469: loss=86.4464, lr=4.754e-05
step 3494: loss=71.8031, lr=4.450e-05
step 3519: loss=68.1519, lr=4.156e-05
train_loss=66.6350  val_loss=43.6649

Epoch 43/50
step 3553: loss=62.1054, lr=3.771e-05
step 3578: loss=60.8088, lr=3.499e-05
step 3603: loss=63.4978, lr=3.237e-05
train_loss=62.4217  val_loss=34.8215
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 44/50
step 3637: loss=56.9632, lr=2.897e-05
step 3662: loss=57.8073, lr=2.658e-05
step 3687: loss=61.6583, lr=2.430e-05
train_loss=61.7242  val_loss=52.3209

Epoch 45/50
step 3721: loss=59.2820, lr=2.135e-05
step 3746: loss=62.4148, lr=1.931e-05
step 3771: loss=62.3417, lr=1.737e-05
train_loss=61.8998  val_loss=64.5755

Epoch 46/50
step 3805: loss=65.4676, lr=1.490e-05
step 3830: loss=80.1590, lr=1.321e-05
step 3855: loss=89.8264, lr=1.163e-05
train_loss=86.2205  val_loss=41.0040

Epoch 47/50
step 3889: loss=276.1094, lr=9.648e-06
step 3914: loss=171.3910, lr=8.320e-06
step 3939: loss=133.3060, lr=7.102e-06
train_loss=127.8233  val_loss=32.3873
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/best.pt

Epoch 48/50
step 3973: loss=59.0458, lr=5.620e-06
step 3998: loss=64.7140, lr=4.661e-06
step 4023: loss=64.1065, lr=3.812e-06
train_loss=65.0220  val_loss=35.6211

Epoch 49/50
step 4057: loss=58.1108, lr=2.837e-06
step 4082: loss=60.1741, lr=2.251e-06
step 4107: loss=57.8385, lr=1.777e-06
train_loss=57.3295  val_loss=58.6922

Epoch 50/50
step 4141: loss=67.3122, lr=1.313e-06
step 4166: loss=72.9193, lr=1.104e-06
step 4191: loss=68.2785, lr=1.007e-06
train_loss=67.3708  val_loss=34.4173

Test loss: 105.2435
Saved final checkpoint to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_014_lr0.0005_hd192_wd1.0_wr0.4/results/metrics_20251124T011843Z.json
