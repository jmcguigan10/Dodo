Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=6788.1026, lr=2.695e-05
step 50: loss=6366.3271, lr=5.190e-05
step 75: loss=5145.3353, lr=7.685e-05
train_loss=5019.9781  val_loss=879.4290
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 2/50
step 109: loss=2126.1956, lr=1.108e-04
step 134: loss=1883.8553, lr=1.357e-04
step 159: loss=1538.2861, lr=1.607e-04
train_loss=1468.6421  val_loss=631.8944
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 3/50
step 193: loss=845.6600, lr=1.946e-04
step 218: loss=912.6337, lr=2.196e-04
step 243: loss=1066.7263, lr=2.445e-04
train_loss=1071.1340  val_loss=454.9291
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 4/50
step 277: loss=894.9174, lr=2.784e-04
step 302: loss=1221.8233, lr=3.034e-04
step 327: loss=1086.5689, lr=3.283e-04
train_loss=1020.2465  val_loss=365.1672
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 5/50
step 361: loss=469.9683, lr=3.623e-04
step 386: loss=463.1086, lr=3.872e-04
step 411: loss=570.6329, lr=4.122e-04
train_loss=553.4203  val_loss=405.6607

Epoch 6/50
step 445: loss=818.9468, lr=4.461e-04
step 470: loss=715.4311, lr=4.711e-04
step 495: loss=670.2179, lr=4.960e-04
train_loss=649.3130  val_loss=547.5126

Epoch 7/50
step 529: loss=533.6254, lr=4.999e-04
step 554: loss=445.3177, lr=4.997e-04
step 579: loss=412.0922, lr=4.994e-04
train_loss=413.4341  val_loss=260.0343
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 8/50
step 613: loss=203.1777, lr=4.989e-04
step 638: loss=205.2338, lr=4.983e-04
step 663: loss=221.0853, lr=4.976e-04
train_loss=234.7030  val_loss=210.0481
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 9/50
step 697: loss=217.8905, lr=4.965e-04
step 722: loss=267.9127, lr=4.956e-04
step 747: loss=237.9143, lr=4.945e-04
train_loss=231.9642  val_loss=143.1750
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 10/50
step 781: loss=203.7975, lr=4.929e-04
step 806: loss=207.9507, lr=4.916e-04
step 831: loss=227.2671, lr=4.902e-04
train_loss=223.3632  val_loss=140.8762
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 11/50
step 865: loss=191.8858, lr=4.881e-04
step 890: loss=178.6713, lr=4.864e-04
step 915: loss=224.7733, lr=4.847e-04
train_loss=224.1424  val_loss=203.7263

Epoch 12/50
step 949: loss=210.0580, lr=4.821e-04
step 974: loss=199.0353, lr=4.801e-04
step 999: loss=192.1022, lr=4.779e-04
train_loss=189.9363  val_loss=119.8539
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 13/50
step 1033: loss=138.8833, lr=4.749e-04
step 1058: loss=160.7224, lr=4.725e-04
step 1083: loss=171.5961, lr=4.701e-04
train_loss=172.0573  val_loss=143.5439

Epoch 14/50
step 1117: loss=164.2758, lr=4.665e-04
step 1142: loss=144.7446, lr=4.638e-04
step 1167: loss=160.4954, lr=4.610e-04
train_loss=161.7561  val_loss=97.4985
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 15/50
step 1201: loss=151.5401, lr=4.571e-04
step 1226: loss=170.1802, lr=4.541e-04
step 1251: loss=167.3815, lr=4.510e-04
train_loss=168.5451  val_loss=184.9494

Epoch 16/50
step 1285: loss=171.1725, lr=4.466e-04
step 1310: loss=174.6612, lr=4.433e-04
step 1335: loss=167.2332, lr=4.399e-04
train_loss=160.4217  val_loss=89.6202
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 17/50
step 1369: loss=121.2996, lr=4.351e-04
step 1394: loss=145.7631, lr=4.315e-04
step 1419: loss=160.9647, lr=4.278e-04
train_loss=163.7625  val_loss=97.3990

Epoch 18/50
step 1453: loss=166.1764, lr=4.227e-04
step 1478: loss=182.5664, lr=4.188e-04
step 1503: loss=169.5555, lr=4.149e-04
train_loss=166.8178  val_loss=94.5166

Epoch 19/50
step 1537: loss=146.3729, lr=4.094e-04
step 1562: loss=171.6727, lr=4.053e-04
step 1587: loss=169.2811, lr=4.011e-04
train_loss=166.0268  val_loss=104.6610

Epoch 20/50
step 1621: loss=136.1108, lr=3.953e-04
step 1646: loss=123.0968, lr=3.909e-04
step 1671: loss=128.2489, lr=3.865e-04
train_loss=127.2920  val_loss=111.4463

Epoch 21/50
step 1705: loss=117.8450, lr=3.804e-04
step 1730: loss=152.5794, lr=3.759e-04
step 1755: loss=140.8684, lr=3.713e-04
train_loss=145.6603  val_loss=80.5692
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 22/50
step 1789: loss=143.4307, lr=3.649e-04
step 1814: loss=141.0500, lr=3.602e-04
step 1839: loss=136.0110, lr=3.554e-04
train_loss=134.3617  val_loss=74.4948
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 23/50
step 1873: loss=235.1653, lr=3.488e-04
step 1898: loss=190.3314, lr=3.439e-04
step 1923: loss=171.1217, lr=3.390e-04
train_loss=164.9664  val_loss=71.6110
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 24/50
step 1957: loss=112.5814, lr=3.322e-04
step 1982: loss=107.5833, lr=3.272e-04
step 2007: loss=109.1168, lr=3.221e-04
train_loss=108.7038  val_loss=105.2181

Epoch 25/50
step 2041: loss=102.8996, lr=3.152e-04
step 2066: loss=104.0967, lr=3.101e-04
step 2091: loss=106.1969, lr=3.049e-04
train_loss=107.4701  val_loss=76.2594

Epoch 26/50
step 2125: loss=126.9572, lr=2.979e-04
step 2150: loss=111.2572, lr=2.927e-04
step 2175: loss=115.6282, lr=2.874e-04
train_loss=115.4346  val_loss=85.9921

Epoch 27/50
step 2209: loss=95.0646, lr=2.803e-04
step 2234: loss=96.5425, lr=2.750e-04
step 2259: loss=99.3665, lr=2.698e-04
train_loss=100.1180  val_loss=59.1436
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 28/50
step 2293: loss=103.3855, lr=2.626e-04
step 2318: loss=103.9776, lr=2.573e-04
step 2343: loss=116.2335, lr=2.520e-04
train_loss=117.4190  val_loss=75.8953

Epoch 29/50
step 2377: loss=128.2668, lr=2.448e-04
step 2402: loss=116.1229, lr=2.395e-04
step 2427: loss=109.9534, lr=2.342e-04
train_loss=106.6407  val_loss=56.4283
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 30/50
step 2461: loss=102.8066, lr=2.270e-04
step 2486: loss=102.9070, lr=2.218e-04
step 2511: loss=102.2604, lr=2.165e-04
train_loss=100.7906  val_loss=71.0414

Epoch 31/50
step 2545: loss=114.5086, lr=2.094e-04
step 2570: loss=106.1771, lr=2.042e-04
step 2595: loss=101.5730, lr=1.990e-04
train_loss=100.3709  val_loss=96.7196

Epoch 32/50
step 2629: loss=97.1018, lr=1.919e-04
step 2654: loss=93.3301, lr=1.868e-04
step 2679: loss=98.1359, lr=1.817e-04
train_loss=99.7127  val_loss=79.5578

Epoch 33/50
step 2713: loss=78.7302, lr=1.748e-04
step 2738: loss=86.4134, lr=1.698e-04
step 2763: loss=85.7952, lr=1.648e-04
train_loss=85.8095  val_loss=72.8707

Epoch 34/50
step 2797: loss=81.2453, lr=1.581e-04
step 2822: loss=86.6746, lr=1.532e-04
step 2847: loss=89.7201, lr=1.483e-04
train_loss=87.8983  val_loss=48.6026
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 35/50
step 2881: loss=71.4332, lr=1.418e-04
step 2906: loss=77.4919, lr=1.370e-04
step 2931: loss=76.2769, lr=1.323e-04
train_loss=75.6072  val_loss=57.1911

Epoch 36/50
step 2965: loss=74.1982, lr=1.261e-04
step 2990: loss=71.7393, lr=1.215e-04
step 3015: loss=77.2960, lr=1.170e-04
train_loss=78.0040  val_loss=50.2117

Epoch 37/50
step 3049: loss=62.7624, lr=1.110e-04
step 3074: loss=66.6007, lr=1.066e-04
step 3099: loss=67.0653, lr=1.023e-04
train_loss=66.5292  val_loss=55.2737

Epoch 38/50
step 3133: loss=71.8212, lr=9.658e-05
step 3158: loss=69.8469, lr=9.244e-05
step 3183: loss=77.9445, lr=8.838e-05
train_loss=75.8841  val_loss=48.4179
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 39/50
step 3217: loss=100.4664, lr=8.298e-05
step 3242: loss=79.4337, lr=7.909e-05
step 3267: loss=83.2842, lr=7.528e-05
train_loss=82.3122  val_loss=46.4096
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 40/50
step 3301: loss=70.6590, lr=7.023e-05
step 3326: loss=72.9520, lr=6.660e-05
step 3351: loss=70.5456, lr=6.307e-05
train_loss=69.5195  val_loss=44.1563
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 41/50
step 3385: loss=65.8200, lr=5.839e-05
step 3410: loss=65.7730, lr=5.506e-05
step 3435: loss=65.7639, lr=5.181e-05
train_loss=64.4915  val_loss=39.5966
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 42/50
step 3469: loss=54.0632, lr=4.754e-05
step 3494: loss=60.6709, lr=4.450e-05
step 3519: loss=60.6705, lr=4.156e-05
train_loss=60.3142  val_loss=48.6719

Epoch 43/50
step 3553: loss=77.6519, lr=3.771e-05
step 3578: loss=69.9006, lr=3.499e-05
step 3603: loss=73.3395, lr=3.237e-05
train_loss=71.6677  val_loss=38.8727
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 44/50
step 3637: loss=63.9977, lr=2.897e-05
step 3662: loss=65.4861, lr=2.658e-05
step 3687: loss=86.2253, lr=2.430e-05
train_loss=83.3875  val_loss=47.6044

Epoch 45/50
step 3721: loss=59.8904, lr=2.135e-05
step 3746: loss=61.9159, lr=1.931e-05
step 3771: loss=62.5822, lr=1.737e-05
train_loss=64.2858  val_loss=42.3206

Epoch 46/50
step 3805: loss=68.0066, lr=1.490e-05
step 3830: loss=66.8273, lr=1.321e-05
step 3855: loss=64.8838, lr=1.163e-05
train_loss=64.8124  val_loss=47.8478

Epoch 47/50
step 3889: loss=61.8887, lr=9.648e-06
step 3914: loss=63.8869, lr=8.320e-06
step 3939: loss=61.8216, lr=7.102e-06
train_loss=60.8151  val_loss=39.7076

Epoch 48/50
step 3973: loss=63.5860, lr=5.620e-06
step 3998: loss=105.6307, lr=4.661e-06
step 4023: loss=87.9160, lr=3.812e-06
train_loss=87.0020  val_loss=36.1123
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/best.pt

Epoch 49/50
step 4057: loss=56.9893, lr=2.837e-06
step 4082: loss=56.9081, lr=2.251e-06
step 4107: loss=56.6518, lr=1.777e-06
train_loss=60.2894  val_loss=38.5370

Epoch 50/50
step 4141: loss=55.1197, lr=1.313e-06
step 4166: loss=61.8357, lr=1.104e-06
step 4191: loss=58.0207, lr=1.007e-06
train_loss=58.7145  val_loss=51.2782

Test loss: 40.3512
Saved final checkpoint to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_021_lr0.0005_hd320_wd1.0_wr0.2/results/metrics_20251124T014646Z.json
