Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=4139.2445, lr=5.295e-05
step 50: loss=4656.8569, lr=1.029e-04
step 75: loss=3847.7664, lr=1.528e-04
train_loss=3573.4842  val_loss=1012.2833
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 2/50
step 109: loss=1665.3515, lr=2.208e-04
step 134: loss=1373.9182, lr=2.707e-04
step 159: loss=1339.6091, lr=3.207e-04
train_loss=1287.6043  val_loss=1274.8601

Epoch 3/50
step 193: loss=644.6163, lr=3.886e-04
step 218: loss=756.6627, lr=4.386e-04
step 243: loss=903.3084, lr=4.885e-04
train_loss=892.9110  val_loss=439.7114
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 4/50
step 277: loss=658.3525, lr=5.564e-04
step 302: loss=665.7106, lr=6.064e-04
step 327: loss=605.2249, lr=6.563e-04
train_loss=590.1850  val_loss=435.5885
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 5/50
step 361: loss=475.2369, lr=7.243e-04
step 386: loss=467.8416, lr=7.742e-04
step 411: loss=475.9879, lr=8.242e-04
train_loss=464.8165  val_loss=252.4133
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 6/50
step 445: loss=393.4973, lr=8.921e-04
step 470: loss=494.8756, lr=9.421e-04
step 495: loss=475.2137, lr=9.920e-04
train_loss=466.8186  val_loss=219.7154
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 7/50
step 529: loss=280.0889, lr=9.998e-04
step 554: loss=298.3768, lr=9.995e-04
step 579: loss=307.8611, lr=9.989e-04
train_loss=312.1482  val_loss=242.6189

Epoch 8/50
step 613: loss=306.6812, lr=9.977e-04
step 638: loss=269.4537, lr=9.966e-04
step 663: loss=261.9109, lr=9.952e-04
train_loss=259.3424  val_loss=249.4748

Epoch 9/50
step 697: loss=242.4606, lr=9.930e-04
step 722: loss=214.2466, lr=9.912e-04
step 747: loss=214.3833, lr=9.891e-04
train_loss=225.9564  val_loss=169.2307
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 10/50
step 781: loss=318.7795, lr=9.859e-04
step 806: loss=350.5545, lr=9.832e-04
step 831: loss=309.9137, lr=9.804e-04
train_loss=294.1713  val_loss=99.5023
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 11/50
step 865: loss=159.6242, lr=9.762e-04
step 890: loss=191.9183, lr=9.729e-04
step 915: loss=180.6950, lr=9.693e-04
train_loss=181.1173  val_loss=139.8627

Epoch 12/50
step 949: loss=187.6056, lr=9.641e-04
step 974: loss=205.7486, lr=9.601e-04
step 999: loss=210.4673, lr=9.558e-04
train_loss=211.7693  val_loss=169.5712

Epoch 13/50
step 1033: loss=136.1157, lr=9.497e-04
step 1058: loss=134.0575, lr=9.450e-04
step 1083: loss=141.0260, lr=9.400e-04
train_loss=140.2571  val_loss=101.3131

Epoch 14/50
step 1117: loss=181.9830, lr=9.330e-04
step 1142: loss=168.5936, lr=9.276e-04
step 1167: loss=161.1218, lr=9.220e-04
train_loss=156.9200  val_loss=150.8908

Epoch 15/50
step 1201: loss=129.6001, lr=9.141e-04
step 1226: loss=116.3505, lr=9.081e-04
step 1251: loss=124.7558, lr=9.018e-04
train_loss=126.1258  val_loss=96.7274
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 16/50
step 1285: loss=171.9777, lr=8.931e-04
step 1310: loss=210.1007, lr=8.865e-04
step 1335: loss=180.8067, lr=8.796e-04
train_loss=172.7275  val_loss=93.5658
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 17/50
step 1369: loss=128.2040, lr=8.701e-04
step 1394: loss=136.7768, lr=8.629e-04
step 1419: loss=152.0091, lr=8.555e-04
train_loss=150.5194  val_loss=92.4269
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 18/50
step 1453: loss=126.2894, lr=8.452e-04
step 1478: loss=132.3986, lr=8.375e-04
step 1503: loss=155.9295, lr=8.296e-04
train_loss=152.5503  val_loss=128.0971

Epoch 19/50
step 1537: loss=108.1400, lr=8.186e-04
step 1562: loss=102.6477, lr=8.103e-04
step 1587: loss=109.5950, lr=8.019e-04
train_loss=112.2587  val_loss=78.8393
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 20/50
step 1621: loss=124.4475, lr=7.903e-04
step 1646: loss=110.7483, lr=7.816e-04
step 1671: loss=112.8525, lr=7.728e-04
train_loss=117.1234  val_loss=94.6553

Epoch 21/50
step 1705: loss=105.0712, lr=7.606e-04
step 1730: loss=106.1954, lr=7.515e-04
step 1755: loss=121.6285, lr=7.423e-04
train_loss=118.3722  val_loss=78.5636
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 22/50
step 1789: loss=112.8558, lr=7.295e-04
step 1814: loss=149.7870, lr=7.201e-04
step 1839: loss=136.6851, lr=7.105e-04
train_loss=135.9481  val_loss=102.2407

Epoch 23/50
step 1873: loss=140.6431, lr=6.973e-04
step 1898: loss=119.9269, lr=6.875e-04
step 1923: loss=143.5772, lr=6.777e-04
train_loss=143.7859  val_loss=76.2491
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 24/50
step 1957: loss=133.2889, lr=6.641e-04
step 1982: loss=149.1948, lr=6.540e-04
step 2007: loss=131.1012, lr=6.439e-04
train_loss=127.9561  val_loss=61.9063
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 25/50
step 2041: loss=91.9466, lr=6.301e-04
step 2066: loss=130.0146, lr=6.198e-04
step 2091: loss=132.3592, lr=6.095e-04
train_loss=129.1446  val_loss=51.2103
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 26/50
step 2125: loss=184.8650, lr=5.953e-04
step 2150: loss=162.0341, lr=5.849e-04
step 2175: loss=142.2392, lr=5.744e-04
train_loss=149.3797  val_loss=65.8711

Epoch 27/50
step 2209: loss=76.5723, lr=5.602e-04
step 2234: loss=85.9307, lr=5.496e-04
step 2259: loss=86.7693, lr=5.391e-04
train_loss=88.3806  val_loss=60.4023

Epoch 28/50
step 2293: loss=119.7005, lr=5.247e-04
step 2318: loss=100.9586, lr=5.141e-04
step 2343: loss=91.7051, lr=5.035e-04
train_loss=91.0031  val_loss=67.3926

Epoch 29/50
step 2377: loss=74.5955, lr=4.890e-04
step 2402: loss=85.0703, lr=4.785e-04
step 2427: loss=87.6076, lr=4.679e-04
train_loss=86.1563  val_loss=48.4668
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 30/50
step 2461: loss=64.5551, lr=4.535e-04
step 2486: loss=73.1060, lr=4.429e-04
step 2511: loss=74.7888, lr=4.324e-04
train_loss=73.7454  val_loss=43.0793
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 31/50
step 2545: loss=91.5845, lr=4.182e-04
step 2570: loss=81.8125, lr=4.077e-04
step 2595: loss=77.3841, lr=3.973e-04
train_loss=75.5553  val_loss=49.9829

Epoch 32/50
step 2629: loss=73.5136, lr=3.833e-04
step 2654: loss=75.6330, lr=3.730e-04
step 2679: loss=73.9172, lr=3.628e-04
train_loss=73.5163  val_loss=51.3285

Epoch 33/50
step 2713: loss=75.2018, lr=3.490e-04
step 2738: loss=80.4518, lr=3.389e-04
step 2763: loss=74.0646, lr=3.289e-04
train_loss=74.9381  val_loss=54.4387

Epoch 34/50
step 2797: loss=89.2495, lr=3.154e-04
step 2822: loss=84.3806, lr=3.056e-04
step 2847: loss=78.6895, lr=2.959e-04
train_loss=77.6751  val_loss=58.2561

Epoch 35/50
step 2881: loss=78.0608, lr=2.828e-04
step 2906: loss=75.0238, lr=2.734e-04
step 2931: loss=74.5731, lr=2.640e-04
train_loss=72.9634  val_loss=49.8611

Epoch 36/50
step 2965: loss=180.9632, lr=2.514e-04
step 2990: loss=204.2912, lr=2.422e-04
step 3015: loss=153.1221, lr=2.332e-04
train_loss=143.9894  val_loss=48.4969

Epoch 37/50
step 3049: loss=62.1245, lr=2.211e-04
step 3074: loss=62.0092, lr=2.124e-04
step 3099: loss=63.3237, lr=2.038e-04
train_loss=66.0311  val_loss=45.3300

Epoch 38/50
step 3133: loss=118.5421, lr=1.923e-04
step 3158: loss=89.3041, lr=1.841e-04
step 3183: loss=77.4737, lr=1.759e-04
train_loss=74.3901  val_loss=40.4755
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 39/50
step 3217: loss=53.4925, lr=1.651e-04
step 3242: loss=64.2167, lr=1.573e-04
step 3267: loss=68.2811, lr=1.497e-04
train_loss=67.4593  val_loss=46.1795

Epoch 40/50
step 3301: loss=52.3788, lr=1.396e-04
step 3326: loss=61.5799, lr=1.323e-04
step 3351: loss=376.1224, lr=1.253e-04
train_loss=343.0611  val_loss=42.0501

Epoch 41/50
step 3385: loss=52.7430, lr=1.159e-04
step 3410: loss=53.3457, lr=1.092e-04
step 3435: loss=51.5133, lr=1.027e-04
train_loss=51.2890  val_loss=46.8044

Epoch 42/50
step 3469: loss=65.8465, lr=9.416e-05
step 3494: loss=66.4921, lr=8.809e-05
step 3519: loss=60.2960, lr=8.220e-05
train_loss=58.2347  val_loss=33.7672
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 43/50
step 3553: loss=63.3843, lr=7.450e-05
step 3578: loss=55.2750, lr=6.906e-05
step 3603: loss=53.1557, lr=6.381e-05
train_loss=51.9167  val_loss=33.5636
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 44/50
step 3637: loss=46.6461, lr=5.699e-05
step 3662: loss=45.4103, lr=5.222e-05
step 3687: loss=43.7976, lr=4.764e-05
train_loss=45.5256  val_loss=34.1509

Epoch 45/50
step 3721: loss=47.9775, lr=4.175e-05
step 3746: loss=46.8334, lr=3.765e-05
step 3771: loss=45.2281, lr=3.377e-05
train_loss=45.4148  val_loss=40.7536

Epoch 46/50
step 3805: loss=42.5301, lr=2.883e-05
step 3830: loss=44.8940, lr=2.545e-05
step 3855: loss=43.7440, lr=2.228e-05
train_loss=43.7098  val_loss=30.7122
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 47/50
step 3889: loss=39.2126, lr=1.831e-05
step 3914: loss=41.2876, lr=1.566e-05
step 3939: loss=41.6849, lr=1.322e-05
train_loss=42.1111  val_loss=44.3086

Epoch 48/50
step 3973: loss=64.0915, lr=1.025e-05
step 3998: loss=53.1235, lr=8.329e-06
step 4023: loss=53.8495, lr=6.630e-06
train_loss=54.3793  val_loss=399.7157

Epoch 49/50
step 4057: loss=43.9830, lr=4.677e-06
step 4082: loss=50.8452, lr=3.505e-06
step 4107: loss=60.0084, lr=2.556e-06
train_loss=57.8515  val_loss=37.3774

Epoch 50/50
step 4141: loss=54.2519, lr=1.627e-06
step 4166: loss=52.0875, lr=1.208e-06
step 4191: loss=54.0168, lr=1.015e-06
train_loss=53.4954  val_loss=45.5863

Test loss: 168.4380
Saved final checkpoint to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_006_lr0.001_hd256_wd1.0_wr0.4/results/metrics_20251124T004704Z.json
