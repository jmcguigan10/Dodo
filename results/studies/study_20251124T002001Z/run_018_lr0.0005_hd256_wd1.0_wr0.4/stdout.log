Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=4164.7538, lr=2.695e-05
step 50: loss=6187.7549, lr=5.190e-05
step 75: loss=5227.6083, lr=7.685e-05
train_loss=4854.1651  val_loss=1061.4246
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 2/50
step 109: loss=2524.6069, lr=1.108e-04
step 134: loss=1931.0752, lr=1.357e-04
step 159: loss=2054.8480, lr=1.607e-04
train_loss=1946.1318  val_loss=849.1492
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 3/50
step 193: loss=812.8370, lr=1.946e-04
step 218: loss=913.7926, lr=2.196e-04
step 243: loss=1024.5986, lr=2.445e-04
train_loss=1059.4405  val_loss=898.0274

Epoch 4/50
step 277: loss=1077.2074, lr=2.784e-04
step 302: loss=1033.6357, lr=3.034e-04
step 327: loss=993.0388, lr=3.283e-04
train_loss=967.5460  val_loss=371.7893
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 5/50
step 361: loss=1023.9710, lr=3.623e-04
step 386: loss=910.1371, lr=3.872e-04
step 411: loss=881.9402, lr=4.122e-04
train_loss=858.3833  val_loss=515.6764

Epoch 6/50
step 445: loss=612.5273, lr=4.461e-04
step 470: loss=667.9834, lr=4.711e-04
step 495: loss=610.1831, lr=4.960e-04
train_loss=607.0841  val_loss=370.2810
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 7/50
step 529: loss=448.3501, lr=4.999e-04
step 554: loss=487.2178, lr=4.997e-04
step 579: loss=515.9340, lr=4.994e-04
train_loss=505.8500  val_loss=318.8710
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 8/50
step 613: loss=334.7228, lr=4.989e-04
step 638: loss=344.1136, lr=4.983e-04
step 663: loss=324.0307, lr=4.976e-04
train_loss=323.3929  val_loss=307.5892
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 9/50
step 697: loss=269.7307, lr=4.965e-04
step 722: loss=259.4480, lr=4.956e-04
step 747: loss=384.0342, lr=4.945e-04
train_loss=378.5120  val_loss=206.9777
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 10/50
step 781: loss=286.1201, lr=4.929e-04
step 806: loss=266.3377, lr=4.916e-04
step 831: loss=321.6982, lr=4.902e-04
train_loss=320.1102  val_loss=207.1803

Epoch 11/50
step 865: loss=330.4651, lr=4.881e-04
step 890: loss=353.4666, lr=4.864e-04
step 915: loss=326.8775, lr=4.847e-04
train_loss=318.6223  val_loss=137.8493
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 12/50
step 949: loss=252.1691, lr=4.821e-04
step 974: loss=233.3696, lr=4.801e-04
step 999: loss=247.3185, lr=4.779e-04
train_loss=251.5016  val_loss=159.2034

Epoch 13/50
step 1033: loss=320.8511, lr=4.749e-04
step 1058: loss=260.6326, lr=4.725e-04
step 1083: loss=231.8887, lr=4.701e-04
train_loss=227.6687  val_loss=130.0636
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 14/50
step 1117: loss=231.2716, lr=4.665e-04
step 1142: loss=239.2549, lr=4.638e-04
step 1167: loss=225.4697, lr=4.610e-04
train_loss=218.3722  val_loss=207.3714

Epoch 15/50
step 1201: loss=194.4382, lr=4.571e-04
step 1226: loss=180.1695, lr=4.541e-04
step 1251: loss=194.3792, lr=4.510e-04
train_loss=195.9320  val_loss=158.2427

Epoch 16/50
step 1285: loss=228.9143, lr=4.466e-04
step 1310: loss=223.9063, lr=4.433e-04
step 1335: loss=214.7176, lr=4.399e-04
train_loss=210.4266  val_loss=132.0227

Epoch 17/50
step 1369: loss=158.8069, lr=4.351e-04
step 1394: loss=154.8803, lr=4.315e-04
step 1419: loss=162.5686, lr=4.278e-04
train_loss=159.8183  val_loss=129.6674
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 18/50
step 1453: loss=130.4982, lr=4.227e-04
step 1478: loss=121.8682, lr=4.188e-04
step 1503: loss=119.4704, lr=4.149e-04
train_loss=119.7236  val_loss=110.4994
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 19/50
step 1537: loss=111.2113, lr=4.094e-04
step 1562: loss=179.7773, lr=4.053e-04
step 1587: loss=171.1266, lr=4.011e-04
train_loss=165.5632  val_loss=103.7859
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 20/50
step 1621: loss=124.5695, lr=3.953e-04
step 1646: loss=112.7249, lr=3.909e-04
step 1671: loss=115.5505, lr=3.865e-04
train_loss=119.4564  val_loss=90.7544
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 21/50
step 1705: loss=126.0761, lr=3.804e-04
step 1730: loss=139.9453, lr=3.759e-04
step 1755: loss=137.2121, lr=3.713e-04
train_loss=134.4510  val_loss=123.2653

Epoch 22/50
step 1789: loss=124.6579, lr=3.649e-04
step 1814: loss=124.3655, lr=3.602e-04
step 1839: loss=117.6158, lr=3.554e-04
train_loss=117.4177  val_loss=94.3451

Epoch 23/50
step 1873: loss=162.5858, lr=3.488e-04
step 1898: loss=167.8693, lr=3.439e-04
step 1923: loss=150.0470, lr=3.390e-04
train_loss=168.4905  val_loss=64.7718
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 24/50
step 1957: loss=98.6814, lr=3.322e-04
step 1982: loss=100.4141, lr=3.272e-04
step 2007: loss=101.1681, lr=3.221e-04
train_loss=100.8713  val_loss=76.3785

Epoch 25/50
step 2041: loss=107.9262, lr=3.152e-04
step 2066: loss=105.6450, lr=3.101e-04
step 2091: loss=109.5393, lr=3.049e-04
train_loss=109.6860  val_loss=71.4950

Epoch 26/50
step 2125: loss=146.1816, lr=2.979e-04
step 2150: loss=135.8523, lr=2.927e-04
step 2175: loss=132.1995, lr=2.874e-04
train_loss=127.5192  val_loss=86.4434

Epoch 27/50
step 2209: loss=97.4989, lr=2.803e-04
step 2234: loss=98.4796, lr=2.750e-04
step 2259: loss=128.5487, lr=2.698e-04
train_loss=131.7336  val_loss=66.9386

Epoch 28/50
step 2293: loss=84.1610, lr=2.626e-04
step 2318: loss=83.2411, lr=2.573e-04
step 2343: loss=101.8125, lr=2.520e-04
train_loss=103.3368  val_loss=111.1664

Epoch 29/50
step 2377: loss=116.9860, lr=2.448e-04
step 2402: loss=112.7826, lr=2.395e-04
step 2427: loss=106.2523, lr=2.342e-04
train_loss=105.3910  val_loss=65.0994

Epoch 30/50
step 2461: loss=87.8492, lr=2.270e-04
step 2486: loss=89.0882, lr=2.218e-04
step 2511: loss=90.9219, lr=2.165e-04
train_loss=90.2497  val_loss=59.5258
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 31/50
step 2545: loss=95.8629, lr=2.094e-04
step 2570: loss=87.9618, lr=2.042e-04
step 2595: loss=98.2529, lr=1.990e-04
train_loss=99.5406  val_loss=73.7441

Epoch 32/50
step 2629: loss=96.7577, lr=1.919e-04
step 2654: loss=91.7694, lr=1.868e-04
step 2679: loss=91.1109, lr=1.817e-04
train_loss=89.8403  val_loss=67.9242

Epoch 33/50
step 2713: loss=90.2325, lr=1.748e-04
step 2738: loss=87.3852, lr=1.698e-04
step 2763: loss=85.0385, lr=1.648e-04
train_loss=83.9324  val_loss=52.0226
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 34/50
step 2797: loss=81.4581, lr=1.581e-04
step 2822: loss=333.2253, lr=1.532e-04
step 2847: loss=249.2625, lr=1.483e-04
train_loss=233.4282  val_loss=46.7178
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 35/50
step 2881: loss=83.6802, lr=1.418e-04
step 2906: loss=100.7322, lr=1.370e-04
step 2931: loss=98.7469, lr=1.323e-04
train_loss=99.2972  val_loss=81.4884

Epoch 36/50
step 2965: loss=103.9684, lr=1.261e-04
step 2990: loss=90.6532, lr=1.215e-04
step 3015: loss=86.3621, lr=1.170e-04
train_loss=86.5979  val_loss=57.1761

Epoch 37/50
step 3049: loss=87.8194, lr=1.110e-04
step 3074: loss=85.1397, lr=1.066e-04
step 3099: loss=78.9332, lr=1.023e-04
train_loss=77.3867  val_loss=39.9628
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/best.pt

Epoch 38/50
step 3133: loss=71.2500, lr=9.658e-05
step 3158: loss=73.9582, lr=9.244e-05
step 3183: loss=77.0367, lr=8.838e-05
train_loss=76.0478  val_loss=289.3277

Epoch 39/50
step 3217: loss=70.7333, lr=8.298e-05
step 3242: loss=75.3160, lr=7.909e-05
step 3267: loss=76.9245, lr=7.528e-05
train_loss=76.3403  val_loss=52.8975

Epoch 40/50
step 3301: loss=67.5831, lr=7.023e-05
step 3326: loss=67.3898, lr=6.660e-05
step 3351: loss=79.9460, lr=6.307e-05
train_loss=78.1802  val_loss=51.6466

Epoch 41/50
step 3385: loss=111.1096, lr=5.839e-05
step 3410: loss=86.2749, lr=5.506e-05
step 3435: loss=80.8462, lr=5.181e-05
train_loss=81.1042  val_loss=41.1788

Epoch 42/50
step 3469: loss=57.8553, lr=4.754e-05
step 3494: loss=62.6284, lr=4.450e-05
step 3519: loss=67.3799, lr=4.156e-05
train_loss=66.2524  val_loss=46.2271

Epoch 43/50
step 3553: loss=71.4810, lr=3.771e-05
step 3578: loss=78.8760, lr=3.499e-05
step 3603: loss=72.2105, lr=3.237e-05
train_loss=71.4937  val_loss=65.6751

Epoch 44/50
step 3637: loss=65.6493, lr=2.897e-05
step 3662: loss=67.1532, lr=2.658e-05
step 3687: loss=65.6271, lr=2.430e-05
train_loss=64.9590  val_loss=45.5170

Epoch 45/50
step 3721: loss=61.6113, lr=2.135e-05
step 3746: loss=64.7638, lr=1.931e-05
step 3771: loss=63.0084, lr=1.737e-05
train_loss=63.3610  val_loss=43.2014

Epoch 46/50
step 3805: loss=66.1660, lr=1.490e-05
step 3830: loss=74.3603, lr=1.321e-05
step 3855: loss=71.3047, lr=1.163e-05
train_loss=69.8056  val_loss=42.7534

Epoch 47/50
step 3889: loss=72.5875, lr=9.648e-06
step 3914: loss=67.9588, lr=8.320e-06
step 3939: loss=67.8190, lr=7.102e-06
train_loss=68.1659  val_loss=42.4303

Epoch 48/50
step 3973: loss=63.6473, lr=5.620e-06
step 3998: loss=63.8215, lr=4.661e-06
step 4023: loss=62.8097, lr=3.812e-06
train_loss=62.2393  val_loss=71.5499

Epoch 49/50
step 4057: loss=58.5338, lr=2.837e-06
step 4082: loss=61.5842, lr=2.251e-06
step 4107: loss=69.6254, lr=1.777e-06
train_loss=67.8152  val_loss=39.9886

Epoch 50/50
step 4141: loss=68.9150, lr=1.313e-06
step 4166: loss=202.6894, lr=1.104e-06
step 4191: loss=158.7303, lr=1.007e-06
train_loss=150.9514  val_loss=44.3346

Test loss: 38.4454
Saved final checkpoint to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_018_lr0.0005_hd256_wd1.0_wr0.4/results/metrics_20251124T013451Z.json
