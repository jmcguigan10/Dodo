Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=4094.7401, lr=2.695e-05
step 50: loss=6171.8065, lr=5.190e-05
step 75: loss=5220.2647, lr=7.685e-05
train_loss=4833.4009  val_loss=1007.7902
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 2/50
step 109: loss=2433.8667, lr=1.108e-04
step 134: loss=1788.5369, lr=1.357e-04
step 159: loss=1547.8777, lr=1.607e-04
train_loss=1485.0590  val_loss=642.9426
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 3/50
step 193: loss=795.4782, lr=1.946e-04
step 218: loss=793.7054, lr=2.196e-04
step 243: loss=831.8429, lr=2.445e-04
train_loss=853.0009  val_loss=413.0135
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 4/50
step 277: loss=883.6557, lr=2.784e-04
step 302: loss=807.3164, lr=3.034e-04
step 327: loss=738.4003, lr=3.283e-04
train_loss=716.4960  val_loss=396.5997
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 5/50
step 361: loss=738.3949, lr=3.623e-04
step 386: loss=623.0307, lr=3.872e-04
step 411: loss=674.3702, lr=4.122e-04
train_loss=642.8173  val_loss=311.9834
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 6/50
step 445: loss=401.7497, lr=4.461e-04
step 470: loss=494.7519, lr=4.711e-04
step 495: loss=434.4301, lr=4.960e-04
train_loss=420.3599  val_loss=210.9395
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 7/50
step 529: loss=310.6058, lr=4.999e-04
step 554: loss=330.4869, lr=4.997e-04
step 579: loss=451.9698, lr=4.994e-04
train_loss=436.6570  val_loss=265.6558

Epoch 8/50
step 613: loss=2395.3126, lr=4.989e-04
step 638: loss=1293.0242, lr=4.983e-04
step 663: loss=936.9091, lr=4.976e-04
train_loss=874.1988  val_loss=139.8915
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 9/50
step 697: loss=248.2899, lr=4.965e-04
step 722: loss=198.4184, lr=4.956e-04
step 747: loss=207.9539, lr=4.945e-04
train_loss=202.3174  val_loss=104.1430
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 10/50
step 781: loss=196.6590, lr=4.929e-04
step 806: loss=175.4442, lr=4.916e-04
step 831: loss=177.1088, lr=4.902e-04
train_loss=175.3571  val_loss=107.1704

Epoch 11/50
step 865: loss=204.8366, lr=4.881e-04
step 890: loss=251.1349, lr=4.864e-04
step 915: loss=232.7416, lr=4.847e-04
train_loss=227.7377  val_loss=82.5359
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 12/50
step 949: loss=132.7659, lr=4.821e-04
step 974: loss=139.0464, lr=4.801e-04
step 999: loss=154.6249, lr=4.779e-04
train_loss=161.2888  val_loss=90.8705

Epoch 13/50
step 1033: loss=195.1967, lr=4.749e-04
step 1058: loss=170.1932, lr=4.725e-04
step 1083: loss=174.8283, lr=4.701e-04
train_loss=172.3551  val_loss=85.8752

Epoch 14/50
step 1117: loss=118.5519, lr=4.665e-04
step 1142: loss=126.5450, lr=4.638e-04
step 1167: loss=130.0608, lr=4.610e-04
train_loss=128.9355  val_loss=113.0623

Epoch 15/50
step 1201: loss=139.6552, lr=4.571e-04
step 1226: loss=129.5683, lr=4.541e-04
step 1251: loss=127.0244, lr=4.510e-04
train_loss=125.4508  val_loss=86.0838

Epoch 16/50
step 1285: loss=110.2360, lr=4.466e-04
step 1310: loss=276.6732, lr=4.433e-04
step 1335: loss=226.9193, lr=4.399e-04
train_loss=222.6902  val_loss=85.8022

Epoch 17/50
step 1369: loss=104.3259, lr=4.351e-04
step 1394: loss=105.1507, lr=4.315e-04
step 1419: loss=117.8210, lr=4.278e-04
train_loss=118.9340  val_loss=107.3968

Epoch 18/50
step 1453: loss=121.0409, lr=4.227e-04
step 1478: loss=115.4745, lr=4.188e-04
step 1503: loss=112.2365, lr=4.149e-04
train_loss=109.9251  val_loss=71.6971
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 19/50
step 1537: loss=101.4681, lr=4.094e-04
step 1562: loss=105.9199, lr=4.053e-04
step 1587: loss=101.2936, lr=4.011e-04
train_loss=99.9968  val_loss=67.7236
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 20/50
step 1621: loss=83.8511, lr=3.953e-04
step 1646: loss=87.1577, lr=3.909e-04
step 1671: loss=92.0746, lr=3.865e-04
train_loss=95.5380  val_loss=89.5232

Epoch 21/50
step 1705: loss=95.4276, lr=3.804e-04
step 1730: loss=106.4959, lr=3.759e-04
step 1755: loss=104.4171, lr=3.713e-04
train_loss=104.7360  val_loss=97.6290

Epoch 22/50
step 1789: loss=133.6546, lr=3.649e-04
step 1814: loss=105.7101, lr=3.602e-04
step 1839: loss=92.4439, lr=3.554e-04
train_loss=90.8765  val_loss=54.9754
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 23/50
step 1873: loss=84.8344, lr=3.488e-04
step 1898: loss=68.8362, lr=3.439e-04
step 1923: loss=67.2915, lr=3.390e-04
train_loss=70.1240  val_loss=51.9627
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 24/50
step 1957: loss=79.4721, lr=3.322e-04
step 1982: loss=86.3439, lr=3.272e-04
step 2007: loss=92.4300, lr=3.221e-04
train_loss=91.5028  val_loss=68.8875

Epoch 25/50
step 2041: loss=86.9854, lr=3.152e-04
step 2066: loss=92.7910, lr=3.101e-04
step 2091: loss=91.1646, lr=3.049e-04
train_loss=88.0466  val_loss=38.6203
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 26/50
step 2125: loss=77.8467, lr=2.979e-04
step 2150: loss=73.4643, lr=2.927e-04
step 2175: loss=73.1815, lr=2.874e-04
train_loss=74.5196  val_loss=95.7538

Epoch 27/50
step 2209: loss=82.7053, lr=2.803e-04
step 2234: loss=78.6837, lr=2.750e-04
step 2259: loss=76.0905, lr=2.698e-04
train_loss=77.1954  val_loss=44.1872

Epoch 28/50
step 2293: loss=135.6496, lr=2.626e-04
step 2318: loss=114.4697, lr=2.573e-04
step 2343: loss=105.9562, lr=2.520e-04
train_loss=103.0045  val_loss=65.6078

Epoch 29/50
step 2377: loss=81.9168, lr=2.448e-04
step 2402: loss=81.3582, lr=2.395e-04
step 2427: loss=79.2677, lr=2.342e-04
train_loss=78.7955  val_loss=47.1029

Epoch 30/50
step 2461: loss=65.6221, lr=2.270e-04
step 2486: loss=71.1695, lr=2.218e-04
step 2511: loss=77.2796, lr=2.165e-04
train_loss=78.3323  val_loss=63.5147

Epoch 31/50
step 2545: loss=87.0875, lr=2.094e-04
step 2570: loss=81.3418, lr=2.042e-04
step 2595: loss=80.0332, lr=1.990e-04
train_loss=80.3742  val_loss=57.1821

Epoch 32/50
step 2629: loss=83.3033, lr=1.919e-04
step 2654: loss=83.7973, lr=1.868e-04
step 2679: loss=78.2207, lr=1.817e-04
train_loss=78.8399  val_loss=41.8078

Epoch 33/50
step 2713: loss=65.4882, lr=1.748e-04
step 2738: loss=64.4880, lr=1.698e-04
step 2763: loss=92.1547, lr=1.648e-04
train_loss=89.6790  val_loss=57.6892

Epoch 34/50
step 2797: loss=75.0546, lr=1.581e-04
step 2822: loss=72.6910, lr=1.532e-04
step 2847: loss=71.0754, lr=1.483e-04
train_loss=71.1435  val_loss=43.2661

Epoch 35/50
step 2881: loss=67.4368, lr=1.418e-04
step 2906: loss=67.4772, lr=1.370e-04
step 2931: loss=67.8392, lr=1.323e-04
train_loss=68.8482  val_loss=54.9004

Epoch 36/50
step 2965: loss=67.5142, lr=1.261e-04
step 2990: loss=73.7207, lr=1.215e-04
step 3015: loss=75.0807, lr=1.170e-04
train_loss=125.0582  val_loss=53.0385

Epoch 37/50
step 3049: loss=66.3868, lr=1.110e-04
step 3074: loss=66.4551, lr=1.066e-04
step 3099: loss=66.0928, lr=1.023e-04
train_loss=65.9868  val_loss=44.8733

Epoch 38/50
step 3133: loss=63.0766, lr=9.658e-05
step 3158: loss=62.6972, lr=9.244e-05
step 3183: loss=66.8809, lr=8.838e-05
train_loss=66.5347  val_loss=43.8697

Epoch 39/50
step 3217: loss=57.7023, lr=8.298e-05
step 3242: loss=62.0002, lr=7.909e-05
step 3267: loss=63.0102, lr=7.528e-05
train_loss=62.0503  val_loss=45.1274

Epoch 40/50
step 3301: loss=55.0242, lr=7.023e-05
step 3326: loss=56.5074, lr=6.660e-05
step 3351: loss=57.5208, lr=6.307e-05
train_loss=57.6284  val_loss=41.0453

Epoch 41/50
step 3385: loss=72.0274, lr=5.839e-05
step 3410: loss=63.9441, lr=5.506e-05
step 3435: loss=61.7548, lr=5.181e-05
train_loss=60.3170  val_loss=37.2253
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 42/50
step 3469: loss=56.8337, lr=4.754e-05
step 3494: loss=58.2086, lr=4.450e-05
step 3519: loss=63.7315, lr=4.156e-05
train_loss=62.0597  val_loss=36.1206
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 43/50
step 3553: loss=61.0348, lr=3.771e-05
step 3578: loss=62.1626, lr=3.499e-05
step 3603: loss=59.4876, lr=3.237e-05
train_loss=58.5306  val_loss=35.5931
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/best.pt

Epoch 44/50
step 3637: loss=74.2851, lr=2.897e-05
step 3662: loss=65.0102, lr=2.658e-05
step 3687: loss=67.1949, lr=2.430e-05
train_loss=69.6581  val_loss=36.8682

Epoch 45/50
step 3721: loss=57.2603, lr=2.135e-05
step 3746: loss=56.0455, lr=1.931e-05
step 3771: loss=55.4002, lr=1.737e-05
train_loss=55.4924  val_loss=38.5953

Epoch 46/50
step 3805: loss=53.0726, lr=1.490e-05
step 3830: loss=55.9252, lr=1.321e-05
step 3855: loss=56.4822, lr=1.163e-05
train_loss=58.0025  val_loss=38.1438

Epoch 47/50
step 3889: loss=58.9165, lr=9.648e-06
step 3914: loss=67.1047, lr=8.320e-06
step 3939: loss=64.6624, lr=7.102e-06
train_loss=65.6284  val_loss=37.8227

Epoch 48/50
step 3973: loss=182.7470, lr=5.620e-06
step 3998: loss=119.1058, lr=4.661e-06
step 4023: loss=118.1271, lr=3.812e-06
train_loss=129.3388  val_loss=48.7378

Epoch 49/50
step 4057: loss=50.7292, lr=2.837e-06
step 4082: loss=60.9200, lr=2.251e-06
step 4107: loss=66.2900, lr=1.777e-06
train_loss=65.4596  val_loss=37.3245

Epoch 50/50
step 4141: loss=151.6915, lr=1.313e-06
step 4166: loss=107.7353, lr=1.104e-06
step 4191: loss=91.2859, lr=1.007e-06
train_loss=90.7198  val_loss=36.4335

Test loss: 31.2748
Saved final checkpoint to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_019_lr0.0005_hd256_wd0.5_wr0.2/results/metrics_20251124T013904Z.json
