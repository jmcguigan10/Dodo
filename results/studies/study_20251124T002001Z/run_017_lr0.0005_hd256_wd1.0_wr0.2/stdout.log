Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=4175.0320, lr=2.695e-05
step 50: loss=6049.8923, lr=5.190e-05
step 75: loss=5022.9296, lr=7.685e-05
train_loss=4653.6256  val_loss=1083.8628
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 2/50
step 109: loss=3594.5766, lr=1.108e-04
step 134: loss=2434.3252, lr=1.357e-04
step 159: loss=2069.4732, lr=1.607e-04
train_loss=1967.5993  val_loss=748.5998
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 3/50
step 193: loss=715.9629, lr=1.946e-04
step 218: loss=854.8653, lr=2.196e-04
step 243: loss=963.4885, lr=2.445e-04
train_loss=1008.9579  val_loss=1205.0594

Epoch 4/50
step 277: loss=989.4362, lr=2.784e-04
step 302: loss=1026.5875, lr=3.034e-04
step 327: loss=903.1199, lr=3.283e-04
train_loss=870.7216  val_loss=574.0065
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 5/50
step 361: loss=789.4853, lr=3.623e-04
step 386: loss=759.0322, lr=3.872e-04
step 411: loss=871.2328, lr=4.122e-04
train_loss=838.0841  val_loss=496.2747
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 6/50
step 445: loss=501.3521, lr=4.461e-04
step 470: loss=505.7086, lr=4.711e-04
step 495: loss=470.7334, lr=4.960e-04
train_loss=465.1014  val_loss=270.8384
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 7/50
step 529: loss=442.6742, lr=4.999e-04
step 554: loss=433.2805, lr=4.997e-04
step 579: loss=456.7072, lr=4.994e-04
train_loss=436.3619  val_loss=225.7351
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 8/50
step 613: loss=267.9214, lr=4.989e-04
step 638: loss=262.7914, lr=4.983e-04
step 663: loss=268.9667, lr=4.976e-04
train_loss=281.2164  val_loss=229.0821

Epoch 9/50
step 697: loss=470.1968, lr=4.965e-04
step 722: loss=367.0375, lr=4.956e-04
step 747: loss=354.4913, lr=4.945e-04
train_loss=346.6306  val_loss=216.6631
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 10/50
step 781: loss=283.9218, lr=4.929e-04
step 806: loss=257.4331, lr=4.916e-04
step 831: loss=263.8441, lr=4.902e-04
train_loss=258.5280  val_loss=159.5417
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 11/50
step 865: loss=293.5112, lr=4.881e-04
step 890: loss=304.6084, lr=4.864e-04
step 915: loss=282.9779, lr=4.847e-04
train_loss=284.7082  val_loss=214.1168

Epoch 12/50
step 949: loss=349.2532, lr=4.821e-04
step 974: loss=277.0391, lr=4.801e-04
step 999: loss=251.4744, lr=4.779e-04
train_loss=251.8590  val_loss=116.5966
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 13/50
step 1033: loss=369.0097, lr=4.749e-04
step 1058: loss=347.1524, lr=4.725e-04
step 1083: loss=308.3037, lr=4.701e-04
train_loss=297.3350  val_loss=114.5365
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 14/50
step 1117: loss=166.3934, lr=4.665e-04
step 1142: loss=189.5358, lr=4.638e-04
step 1167: loss=198.5901, lr=4.610e-04
train_loss=195.2548  val_loss=191.8742

Epoch 15/50
step 1201: loss=174.7170, lr=4.571e-04
step 1226: loss=169.9347, lr=4.541e-04
step 1251: loss=176.6212, lr=4.510e-04
train_loss=179.5523  val_loss=136.7510

Epoch 16/50
step 1285: loss=155.1345, lr=4.466e-04
step 1310: loss=173.9368, lr=4.433e-04
step 1335: loss=169.4997, lr=4.399e-04
train_loss=165.8436  val_loss=106.5460
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 17/50
step 1369: loss=121.7324, lr=4.351e-04
step 1394: loss=127.3935, lr=4.315e-04
step 1419: loss=146.5557, lr=4.278e-04
train_loss=150.2156  val_loss=145.1839

Epoch 18/50
step 1453: loss=196.7847, lr=4.227e-04
step 1478: loss=176.4742, lr=4.188e-04
step 1503: loss=173.0121, lr=4.149e-04
train_loss=170.9006  val_loss=131.3055

Epoch 19/50
step 1537: loss=192.4944, lr=4.094e-04
step 1562: loss=182.5111, lr=4.053e-04
step 1587: loss=194.3742, lr=4.011e-04
train_loss=198.3902  val_loss=89.6469
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 20/50
step 1621: loss=144.6134, lr=3.953e-04
step 1646: loss=153.4217, lr=3.909e-04
step 1671: loss=151.0479, lr=3.865e-04
train_loss=160.0281  val_loss=134.5003

Epoch 21/50
step 1705: loss=198.1906, lr=3.804e-04
step 1730: loss=191.7197, lr=3.759e-04
step 1755: loss=181.2906, lr=3.713e-04
train_loss=181.1919  val_loss=146.1666

Epoch 22/50
step 1789: loss=184.5736, lr=3.649e-04
step 1814: loss=173.8316, lr=3.602e-04
step 1839: loss=161.4719, lr=3.554e-04
train_loss=161.2118  val_loss=115.0253

Epoch 23/50
step 1873: loss=150.0108, lr=3.488e-04
step 1898: loss=139.6770, lr=3.439e-04
step 1923: loss=134.3532, lr=3.390e-04
train_loss=140.9537  val_loss=78.9149
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 24/50
step 1957: loss=124.5044, lr=3.322e-04
step 1982: loss=139.5677, lr=3.272e-04
step 2007: loss=141.9525, lr=3.221e-04
train_loss=139.6785  val_loss=80.3191

Epoch 25/50
step 2041: loss=112.3025, lr=3.152e-04
step 2066: loss=106.7634, lr=3.101e-04
step 2091: loss=121.9810, lr=3.049e-04
train_loss=121.1572  val_loss=70.5864
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 26/50
step 2125: loss=185.1474, lr=2.979e-04
step 2150: loss=141.6685, lr=2.927e-04
step 2175: loss=138.7348, lr=2.874e-04
train_loss=134.5520  val_loss=97.4056

Epoch 27/50
step 2209: loss=113.3416, lr=2.803e-04
step 2234: loss=104.9582, lr=2.750e-04
step 2259: loss=100.8566, lr=2.698e-04
train_loss=100.6272  val_loss=79.0582

Epoch 28/50
step 2293: loss=83.1244, lr=2.626e-04
step 2318: loss=85.2306, lr=2.573e-04
step 2343: loss=102.7280, lr=2.520e-04
train_loss=104.1341  val_loss=102.8420

Epoch 29/50
step 2377: loss=134.4472, lr=2.448e-04
step 2402: loss=111.6629, lr=2.395e-04
step 2427: loss=101.9388, lr=2.342e-04
train_loss=101.8851  val_loss=66.4501
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 30/50
step 2461: loss=89.0372, lr=2.270e-04
step 2486: loss=95.5273, lr=2.218e-04
step 2511: loss=93.4698, lr=2.165e-04
train_loss=92.6527  val_loss=54.0406
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 31/50
step 2545: loss=91.5758, lr=2.094e-04
step 2570: loss=78.9055, lr=2.042e-04
step 2595: loss=79.2796, lr=1.990e-04
train_loss=80.3709  val_loss=64.8012

Epoch 32/50
step 2629: loss=94.7375, lr=1.919e-04
step 2654: loss=94.6490, lr=1.868e-04
step 2679: loss=88.6346, lr=1.817e-04
train_loss=90.3004  val_loss=68.1432

Epoch 33/50
step 2713: loss=80.2418, lr=1.748e-04
step 2738: loss=81.0593, lr=1.698e-04
step 2763: loss=81.3354, lr=1.648e-04
train_loss=80.0238  val_loss=51.0212
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 34/50
step 2797: loss=72.1011, lr=1.581e-04
step 2822: loss=72.7890, lr=1.532e-04
step 2847: loss=80.7646, lr=1.483e-04
train_loss=78.1622  val_loss=47.5518
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 35/50
step 2881: loss=67.0901, lr=1.418e-04
step 2906: loss=65.7329, lr=1.370e-04
step 2931: loss=63.4277, lr=1.323e-04
train_loss=64.3129  val_loss=66.3818

Epoch 36/50
step 2965: loss=69.7505, lr=1.261e-04
step 2990: loss=65.8617, lr=1.215e-04
step 3015: loss=65.5122, lr=1.170e-04
train_loss=65.0278  val_loss=52.0326

Epoch 37/50
step 3049: loss=63.3205, lr=1.110e-04
step 3074: loss=67.7895, lr=1.066e-04
step 3099: loss=66.4644, lr=1.023e-04
train_loss=65.1082  val_loss=50.3814

Epoch 38/50
step 3133: loss=113.0124, lr=9.658e-05
step 3158: loss=98.0971, lr=9.244e-05
step 3183: loss=88.9801, lr=8.838e-05
train_loss=86.8385  val_loss=50.9166

Epoch 39/50
step 3217: loss=94.8558, lr=8.298e-05
step 3242: loss=96.8912, lr=7.909e-05
step 3267: loss=93.8322, lr=7.528e-05
train_loss=91.1518  val_loss=50.8724

Epoch 40/50
step 3301: loss=68.9454, lr=7.023e-05
step 3326: loss=75.7263, lr=6.660e-05
step 3351: loss=76.6363, lr=6.307e-05
train_loss=76.1359  val_loss=63.9919

Epoch 41/50
step 3385: loss=78.6886, lr=5.839e-05
step 3410: loss=72.6234, lr=5.506e-05
step 3435: loss=72.6644, lr=5.181e-05
train_loss=74.0687  val_loss=46.1139
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 42/50
step 3469: loss=79.3601, lr=4.754e-05
step 3494: loss=77.9139, lr=4.450e-05
step 3519: loss=93.6646, lr=4.156e-05
train_loss=90.7380  val_loss=53.1259

Epoch 43/50
step 3553: loss=88.4515, lr=3.771e-05
step 3578: loss=158.2250, lr=3.499e-05
step 3603: loss=128.0588, lr=3.237e-05
train_loss=126.7610  val_loss=44.2471
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 44/50
step 3637: loss=62.5387, lr=2.897e-05
step 3662: loss=62.0509, lr=2.658e-05
step 3687: loss=62.9206, lr=2.430e-05
train_loss=62.8510  val_loss=45.3365

Epoch 45/50
step 3721: loss=70.7958, lr=2.135e-05
step 3746: loss=71.9711, lr=1.931e-05
step 3771: loss=74.9947, lr=1.737e-05
train_loss=73.3480  val_loss=83.4871

Epoch 46/50
step 3805: loss=81.8857, lr=1.490e-05
step 3830: loss=77.9820, lr=1.321e-05
step 3855: loss=71.6336, lr=1.163e-05
train_loss=70.4151  val_loss=48.2016

Epoch 47/50
step 3889: loss=69.3335, lr=9.648e-06
step 3914: loss=78.0858, lr=8.320e-06
step 3939: loss=479.4396, lr=7.102e-06
train_loss=439.0779  val_loss=40.2175
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 48/50
step 3973: loss=66.8637, lr=5.620e-06
step 3998: loss=66.0235, lr=4.661e-06
step 4023: loss=67.2555, lr=3.812e-06
train_loss=66.8052  val_loss=39.6936
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/best.pt

Epoch 49/50
step 4057: loss=68.5401, lr=2.837e-06
step 4082: loss=66.5513, lr=2.251e-06
step 4107: loss=82.1893, lr=1.777e-06
train_loss=79.6677  val_loss=42.3597

Epoch 50/50
step 4141: loss=59.6537, lr=1.313e-06
step 4166: loss=59.8026, lr=1.104e-06
step 4191: loss=62.1315, lr=1.007e-06
train_loss=63.4127  val_loss=50.4083

Test loss: 38.3205
Saved final checkpoint to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_017_lr0.0005_hd256_wd1.0_wr0.2/results/metrics_20251124T013053Z.json
