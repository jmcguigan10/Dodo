Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=6713.3204, lr=2.695e-05
step 50: loss=6087.3152, lr=5.190e-05
step 75: loss=4832.5458, lr=7.685e-05
train_loss=4576.7358  val_loss=1032.9935
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 2/50
step 109: loss=1767.2335, lr=1.108e-04
step 134: loss=1629.5981, lr=1.357e-04
step 159: loss=1326.7494, lr=1.607e-04
train_loss=1262.8748  val_loss=443.3587
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 3/50
step 193: loss=591.4202, lr=1.946e-04
step 218: loss=642.8538, lr=2.196e-04
step 243: loss=795.0719, lr=2.445e-04
train_loss=762.0938  val_loss=820.5746

Epoch 4/50
step 277: loss=589.3810, lr=2.784e-04
step 302: loss=791.2272, lr=3.034e-04
step 327: loss=830.6556, lr=3.283e-04
train_loss=794.6453  val_loss=462.6988

Epoch 5/50
step 361: loss=485.0062, lr=3.623e-04
step 386: loss=479.4709, lr=3.872e-04
step 411: loss=580.6823, lr=4.122e-04
train_loss=563.4190  val_loss=288.3528
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 6/50
step 445: loss=924.6230, lr=4.461e-04
step 470: loss=689.5587, lr=4.711e-04
step 495: loss=566.6090, lr=4.960e-04
train_loss=537.3390  val_loss=790.0491

Epoch 7/50
step 529: loss=534.8342, lr=4.999e-04
step 554: loss=435.1856, lr=4.997e-04
step 579: loss=382.9245, lr=4.994e-04
train_loss=397.6764  val_loss=240.8247
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 8/50
step 613: loss=265.4212, lr=4.989e-04
step 638: loss=240.3160, lr=4.983e-04
step 663: loss=241.9171, lr=4.976e-04
train_loss=240.0446  val_loss=185.0172
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 9/50
step 697: loss=207.5998, lr=4.965e-04
step 722: loss=218.3689, lr=4.956e-04
step 747: loss=214.2101, lr=4.945e-04
train_loss=212.1258  val_loss=210.3228

Epoch 10/50
step 781: loss=207.7469, lr=4.929e-04
step 806: loss=192.4624, lr=4.916e-04
step 831: loss=188.9795, lr=4.902e-04
train_loss=181.5297  val_loss=84.1307
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 11/50
step 865: loss=143.9248, lr=4.881e-04
step 890: loss=160.4439, lr=4.864e-04
step 915: loss=176.1371, lr=4.847e-04
train_loss=171.2676  val_loss=123.4558

Epoch 12/50
step 949: loss=117.8164, lr=4.821e-04
step 974: loss=122.7029, lr=4.801e-04
step 999: loss=127.1235, lr=4.779e-04
train_loss=133.5936  val_loss=121.9160

Epoch 13/50
step 1033: loss=183.8911, lr=4.749e-04
step 1058: loss=172.6337, lr=4.725e-04
step 1083: loss=188.9840, lr=4.701e-04
train_loss=185.8863  val_loss=123.0536

Epoch 14/50
step 1117: loss=159.1814, lr=4.665e-04
step 1142: loss=137.0074, lr=4.638e-04
step 1167: loss=140.7054, lr=4.610e-04
train_loss=179.1993  val_loss=116.2354

Epoch 15/50
step 1201: loss=102.7068, lr=4.571e-04
step 1226: loss=115.9364, lr=4.541e-04
step 1251: loss=130.8170, lr=4.510e-04
train_loss=129.0417  val_loss=83.9206
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 16/50
step 1285: loss=107.0815, lr=4.466e-04
step 1310: loss=113.9528, lr=4.433e-04
step 1335: loss=111.9962, lr=4.399e-04
train_loss=114.2666  val_loss=156.3381

Epoch 17/50
step 1369: loss=107.9465, lr=4.351e-04
step 1394: loss=106.2937, lr=4.315e-04
step 1419: loss=116.8629, lr=4.278e-04
train_loss=121.2466  val_loss=97.0688

Epoch 18/50
step 1453: loss=104.6657, lr=4.227e-04
step 1478: loss=122.2449, lr=4.188e-04
step 1503: loss=121.4983, lr=4.149e-04
train_loss=119.9331  val_loss=94.6902

Epoch 19/50
step 1537: loss=109.3960, lr=4.094e-04
step 1562: loss=109.8843, lr=4.053e-04
step 1587: loss=128.1559, lr=4.011e-04
train_loss=126.8760  val_loss=83.8027
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 20/50
step 1621: loss=120.0395, lr=3.953e-04
step 1646: loss=107.5361, lr=3.909e-04
step 1671: loss=110.5809, lr=3.865e-04
train_loss=110.5861  val_loss=81.4020
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 21/50
step 1705: loss=98.1933, lr=3.804e-04
step 1730: loss=97.8330, lr=3.759e-04
step 1755: loss=90.7991, lr=3.713e-04
train_loss=98.0709  val_loss=56.8235
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 22/50
step 1789: loss=128.3395, lr=3.649e-04
step 1814: loss=189.1029, lr=3.602e-04
step 1839: loss=159.0179, lr=3.554e-04
train_loss=150.9733  val_loss=64.7196

Epoch 23/50
step 1873: loss=113.9870, lr=3.488e-04
step 1898: loss=107.2974, lr=3.439e-04
step 1923: loss=105.3087, lr=3.390e-04
train_loss=214.3824  val_loss=59.7807

Epoch 24/50
step 1957: loss=74.8724, lr=3.322e-04
step 1982: loss=73.9830, lr=3.272e-04
step 2007: loss=76.9639, lr=3.221e-04
train_loss=77.6842  val_loss=87.2953

Epoch 25/50
step 2041: loss=100.1040, lr=3.152e-04
step 2066: loss=93.6332, lr=3.101e-04
step 2091: loss=92.5928, lr=3.049e-04
train_loss=92.5399  val_loss=67.6747

Epoch 26/50
step 2125: loss=151.5765, lr=2.979e-04
step 2150: loss=131.4015, lr=2.927e-04
step 2175: loss=117.6428, lr=2.874e-04
train_loss=114.6340  val_loss=77.9132

Epoch 27/50
step 2209: loss=84.7418, lr=2.803e-04
step 2234: loss=82.7778, lr=2.750e-04
step 2259: loss=80.5024, lr=2.698e-04
train_loss=79.4991  val_loss=67.0948

Epoch 28/50
step 2293: loss=91.9691, lr=2.626e-04
step 2318: loss=127.5406, lr=2.573e-04
step 2343: loss=111.3195, lr=2.520e-04
train_loss=124.5990  val_loss=55.8386
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 29/50
step 2377: loss=74.7047, lr=2.448e-04
step 2402: loss=81.8536, lr=2.395e-04
step 2427: loss=80.9541, lr=2.342e-04
train_loss=79.5343  val_loss=48.2134
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 30/50
step 2461: loss=71.2892, lr=2.270e-04
step 2486: loss=74.0459, lr=2.218e-04
step 2511: loss=73.6504, lr=2.165e-04
train_loss=73.1323  val_loss=44.2015
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 31/50
step 2545: loss=206.4531, lr=2.094e-04
step 2570: loss=142.2044, lr=2.042e-04
step 2595: loss=122.3913, lr=1.990e-04
train_loss=117.1884  val_loss=50.5190

Epoch 32/50
step 2629: loss=74.8033, lr=1.919e-04
step 2654: loss=76.4193, lr=1.868e-04
step 2679: loss=78.7426, lr=1.817e-04
train_loss=78.4800  val_loss=52.4757

Epoch 33/50
step 2713: loss=68.4817, lr=1.748e-04
step 2738: loss=75.5395, lr=1.698e-04
step 2763: loss=75.0536, lr=1.648e-04
train_loss=77.3346  val_loss=56.1724

Epoch 34/50
step 2797: loss=83.7123, lr=1.581e-04
step 2822: loss=97.5973, lr=1.532e-04
step 2847: loss=130.4898, lr=1.483e-04
train_loss=126.9011  val_loss=48.9913

Epoch 35/50
step 2881: loss=74.3339, lr=1.418e-04
step 2906: loss=87.2737, lr=1.370e-04
step 2931: loss=85.0663, lr=1.323e-04
train_loss=84.4172  val_loss=68.0750

Epoch 36/50
step 2965: loss=78.5095, lr=1.261e-04
step 2990: loss=76.9478, lr=1.215e-04
step 3015: loss=73.9083, lr=1.170e-04
train_loss=74.8278  val_loss=41.0756
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 37/50
step 3049: loss=69.5353, lr=1.110e-04
step 3074: loss=74.7568, lr=1.066e-04
step 3099: loss=77.6421, lr=1.023e-04
train_loss=76.8487  val_loss=49.8788

Epoch 38/50
step 3133: loss=61.2276, lr=9.658e-05
step 3158: loss=81.2413, lr=9.244e-05
step 3183: loss=76.1558, lr=8.838e-05
train_loss=74.7900  val_loss=45.1138

Epoch 39/50
step 3217: loss=64.3857, lr=8.298e-05
step 3242: loss=67.6228, lr=7.909e-05
step 3267: loss=74.4111, lr=7.528e-05
train_loss=74.9065  val_loss=69.5854

Epoch 40/50
step 3301: loss=75.4040, lr=7.023e-05
step 3326: loss=73.7387, lr=6.660e-05
step 3351: loss=71.6143, lr=6.307e-05
train_loss=72.4359  val_loss=42.7926

Epoch 41/50
step 3385: loss=59.5098, lr=5.839e-05
step 3410: loss=64.4161, lr=5.506e-05
step 3435: loss=63.3218, lr=5.181e-05
train_loss=62.0522  val_loss=43.2661

Epoch 42/50
step 3469: loss=56.4787, lr=4.754e-05
step 3494: loss=62.4564, lr=4.450e-05
step 3519: loss=60.4451, lr=4.156e-05
train_loss=60.2859  val_loss=38.7568
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 43/50
step 3553: loss=71.5963, lr=3.771e-05
step 3578: loss=62.9309, lr=3.499e-05
step 3603: loss=62.8922, lr=3.237e-05
train_loss=64.1906  val_loss=33.5705
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/best.pt

Epoch 44/50
step 3637: loss=63.5149, lr=2.897e-05
step 3662: loss=61.4882, lr=2.658e-05
step 3687: loss=59.8370, lr=2.430e-05
train_loss=60.3200  val_loss=41.4579

Epoch 45/50
step 3721: loss=50.9274, lr=2.135e-05
step 3746: loss=61.0390, lr=1.931e-05
step 3771: loss=58.5321, lr=1.737e-05
train_loss=59.3512  val_loss=40.5201

Epoch 46/50
step 3805: loss=51.0966, lr=1.490e-05
step 3830: loss=58.1606, lr=1.321e-05
step 3855: loss=57.5192, lr=1.163e-05
train_loss=57.6846  val_loss=42.3492

Epoch 47/50
step 3889: loss=60.5431, lr=9.648e-06
step 3914: loss=58.4762, lr=8.320e-06
step 3939: loss=59.4705, lr=7.102e-06
train_loss=58.6373  val_loss=48.1642

Epoch 48/50
step 3973: loss=53.1525, lr=5.620e-06
step 3998: loss=51.6362, lr=4.661e-06
step 4023: loss=51.3910, lr=3.812e-06
train_loss=52.4935  val_loss=35.9303

Epoch 49/50
step 4057: loss=64.8069, lr=2.837e-06
step 4082: loss=61.3116, lr=2.251e-06
step 4107: loss=59.4404, lr=1.777e-06
train_loss=63.1263  val_loss=40.8710

Epoch 50/50
step 4141: loss=55.8411, lr=1.313e-06
step 4166: loss=58.4739, lr=1.104e-06
step 4191: loss=56.5223, lr=1.007e-06
train_loss=57.2054  val_loss=41.4489

Test loss: 38.6129
Saved final checkpoint to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_024_lr0.0005_hd320_wd0.5_wr0.4/results/metrics_20251124T015833Z.json
