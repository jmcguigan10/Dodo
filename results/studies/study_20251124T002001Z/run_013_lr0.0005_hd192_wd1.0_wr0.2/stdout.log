Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=12559.6508, lr=2.695e-05
step 50: loss=9492.6666, lr=5.190e-05
step 75: loss=8485.6898, lr=7.685e-05
train_loss=8401.3401  val_loss=1579.8593
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 2/50
step 109: loss=1704.3318, lr=1.108e-04
step 134: loss=1391.6610, lr=1.357e-04
step 159: loss=1384.7693, lr=1.607e-04
train_loss=1336.0400  val_loss=785.2523
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 3/50
step 193: loss=1144.1939, lr=1.946e-04
step 218: loss=1313.0337, lr=2.196e-04
step 243: loss=1177.2719, lr=2.445e-04
train_loss=1112.5219  val_loss=455.7977
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 4/50
step 277: loss=1101.9400, lr=2.784e-04
step 302: loss=970.8587, lr=3.034e-04
step 327: loss=949.2283, lr=3.283e-04
train_loss=923.8600  val_loss=553.8817

Epoch 5/50
step 361: loss=532.1759, lr=3.623e-04
step 386: loss=680.2311, lr=3.872e-04
step 411: loss=704.1796, lr=4.122e-04
train_loss=674.7340  val_loss=348.2945
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 6/50
step 445: loss=494.8891, lr=4.461e-04
step 470: loss=705.3582, lr=4.711e-04
step 495: loss=599.6006, lr=4.960e-04
train_loss=574.4108  val_loss=225.4202
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 7/50
step 529: loss=343.5865, lr=4.999e-04
step 554: loss=385.5628, lr=4.997e-04
step 579: loss=395.1052, lr=4.994e-04
train_loss=388.6643  val_loss=184.0346
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 8/50
step 613: loss=388.1863, lr=4.989e-04
step 638: loss=390.2518, lr=4.983e-04
step 663: loss=400.2730, lr=4.976e-04
train_loss=389.7772  val_loss=257.5224

Epoch 9/50
step 697: loss=272.0815, lr=4.965e-04
step 722: loss=283.3685, lr=4.956e-04
step 747: loss=286.7147, lr=4.945e-04
train_loss=286.4313  val_loss=192.3893

Epoch 10/50
step 781: loss=264.7835, lr=4.929e-04
step 806: loss=255.0745, lr=4.916e-04
step 831: loss=266.7990, lr=4.902e-04
train_loss=269.3106  val_loss=161.0683
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 11/50
step 865: loss=188.6512, lr=4.881e-04
step 890: loss=200.3137, lr=4.864e-04
step 915: loss=207.9749, lr=4.847e-04
train_loss=205.9600  val_loss=168.6320

Epoch 12/50
step 949: loss=209.5056, lr=4.821e-04
step 974: loss=203.4851, lr=4.801e-04
step 999: loss=185.3932, lr=4.779e-04
train_loss=182.9729  val_loss=124.3136
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 13/50
step 1033: loss=162.4978, lr=4.749e-04
step 1058: loss=161.3081, lr=4.725e-04
step 1083: loss=150.9773, lr=4.701e-04
train_loss=150.0084  val_loss=111.9937
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 14/50
step 1117: loss=125.4151, lr=4.665e-04
step 1142: loss=141.5372, lr=4.638e-04
step 1167: loss=145.8203, lr=4.610e-04
train_loss=145.6769  val_loss=77.1097
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 15/50
step 1201: loss=109.5243, lr=4.571e-04
step 1226: loss=120.1600, lr=4.541e-04
step 1251: loss=133.1311, lr=4.510e-04
train_loss=132.9805  val_loss=95.5230

Epoch 16/50
step 1285: loss=144.3718, lr=4.466e-04
step 1310: loss=130.4219, lr=4.433e-04
step 1335: loss=128.2761, lr=4.399e-04
train_loss=128.1084  val_loss=102.5915

Epoch 17/50
step 1369: loss=225.3373, lr=4.351e-04
step 1394: loss=167.5579, lr=4.315e-04
step 1419: loss=167.9661, lr=4.278e-04
train_loss=162.1526  val_loss=67.7943
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 18/50
step 1453: loss=114.0778, lr=4.227e-04
step 1478: loss=124.2763, lr=4.188e-04
step 1503: loss=120.0933, lr=4.149e-04
train_loss=119.1966  val_loss=114.5961

Epoch 19/50
step 1537: loss=145.3599, lr=4.094e-04
step 1562: loss=170.6171, lr=4.053e-04
step 1587: loss=151.3191, lr=4.011e-04
train_loss=146.7639  val_loss=107.0744

Epoch 20/50
step 1621: loss=122.8835, lr=3.953e-04
step 1646: loss=118.7515, lr=3.909e-04
step 1671: loss=115.0171, lr=3.865e-04
train_loss=116.2176  val_loss=69.5762

Epoch 21/50
step 1705: loss=151.1849, lr=3.804e-04
step 1730: loss=129.3232, lr=3.759e-04
step 1755: loss=128.1088, lr=3.713e-04
train_loss=125.3902  val_loss=67.1036
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 22/50
step 1789: loss=90.3243, lr=3.649e-04
step 1814: loss=117.1655, lr=3.602e-04
step 1839: loss=112.3762, lr=3.554e-04
train_loss=111.1324  val_loss=84.3413

Epoch 23/50
step 1873: loss=121.5082, lr=3.488e-04
step 1898: loss=133.4425, lr=3.439e-04
step 1923: loss=129.8265, lr=3.390e-04
train_loss=127.4899  val_loss=78.1548

Epoch 24/50
step 1957: loss=105.7313, lr=3.322e-04
step 1982: loss=110.4414, lr=3.272e-04
step 2007: loss=103.9019, lr=3.221e-04
train_loss=102.0742  val_loss=70.4245

Epoch 25/50
step 2041: loss=94.6568, lr=3.152e-04
step 2066: loss=104.2490, lr=3.101e-04
step 2091: loss=98.3453, lr=3.049e-04
train_loss=95.9884  val_loss=61.6877
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 26/50
step 2125: loss=96.6716, lr=2.979e-04
step 2150: loss=137.3084, lr=2.927e-04
step 2175: loss=122.3486, lr=2.874e-04
train_loss=120.5854  val_loss=70.3595

Epoch 27/50
step 2209: loss=110.3092, lr=2.803e-04
step 2234: loss=110.9008, lr=2.750e-04
step 2259: loss=103.1182, lr=2.698e-04
train_loss=101.3201  val_loss=56.2856
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 28/50
step 2293: loss=76.4029, lr=2.626e-04
step 2318: loss=83.1525, lr=2.573e-04
step 2343: loss=81.8096, lr=2.520e-04
train_loss=81.7989  val_loss=62.7711

Epoch 29/50
step 2377: loss=81.3954, lr=2.448e-04
step 2402: loss=81.2568, lr=2.395e-04
step 2427: loss=80.0589, lr=2.342e-04
train_loss=79.6129  val_loss=59.8353

Epoch 30/50
step 2461: loss=95.2415, lr=2.270e-04
step 2486: loss=83.3742, lr=2.218e-04
step 2511: loss=77.4446, lr=2.165e-04
train_loss=75.3444  val_loss=53.3784
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 31/50
step 2545: loss=69.2402, lr=2.094e-04
step 2570: loss=76.6881, lr=2.042e-04
step 2595: loss=71.8336, lr=1.990e-04
train_loss=70.6706  val_loss=60.9341

Epoch 32/50
step 2629: loss=56.7758, lr=1.919e-04
step 2654: loss=68.0899, lr=1.868e-04
step 2679: loss=70.1547, lr=1.817e-04
train_loss=70.3746  val_loss=42.3170
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 33/50
step 2713: loss=63.5897, lr=1.748e-04
step 2738: loss=72.5293, lr=1.698e-04
step 2763: loss=69.0949, lr=1.648e-04
train_loss=70.3050  val_loss=60.3886

Epoch 34/50
step 2797: loss=73.8760, lr=1.581e-04
step 2822: loss=147.6926, lr=1.532e-04
step 2847: loss=127.5264, lr=1.483e-04
train_loss=121.4072  val_loss=48.3059

Epoch 35/50
step 2881: loss=77.3010, lr=1.418e-04
step 2906: loss=83.0476, lr=1.370e-04
step 2931: loss=83.2647, lr=1.323e-04
train_loss=82.9717  val_loss=60.4248

Epoch 36/50
step 2965: loss=75.3582, lr=1.261e-04
step 2990: loss=71.7771, lr=1.215e-04
step 3015: loss=77.1894, lr=1.170e-04
train_loss=76.6141  val_loss=50.6984

Epoch 37/50
step 3049: loss=62.9013, lr=1.110e-04
step 3074: loss=61.0395, lr=1.066e-04
step 3099: loss=64.1647, lr=1.023e-04
train_loss=63.6999  val_loss=51.3781

Epoch 38/50
step 3133: loss=62.5116, lr=9.658e-05
step 3158: loss=60.1857, lr=9.244e-05
step 3183: loss=63.5252, lr=8.838e-05
train_loss=63.2192  val_loss=41.7551
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 39/50
step 3217: loss=68.7460, lr=8.298e-05
step 3242: loss=66.1938, lr=7.909e-05
step 3267: loss=68.1729, lr=7.528e-05
train_loss=66.5730  val_loss=47.3801

Epoch 40/50
step 3301: loss=70.4760, lr=7.023e-05
step 3326: loss=69.6945, lr=6.660e-05
step 3351: loss=63.9643, lr=6.307e-05
train_loss=62.6396  val_loss=37.9050
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 41/50
step 3385: loss=63.6523, lr=5.839e-05
step 3410: loss=64.9425, lr=5.506e-05
step 3435: loss=72.0127, lr=5.181e-05
train_loss=72.2711  val_loss=49.7425

Epoch 42/50
step 3469: loss=66.1800, lr=4.754e-05
step 3494: loss=61.6218, lr=4.450e-05
step 3519: loss=60.2838, lr=4.156e-05
train_loss=60.7807  val_loss=39.5901

Epoch 43/50
step 3553: loss=63.4387, lr=3.771e-05
step 3578: loss=65.1876, lr=3.499e-05
step 3603: loss=63.2275, lr=3.237e-05
train_loss=66.8277  val_loss=51.5299

Epoch 44/50
step 3637: loss=56.6013, lr=2.897e-05
step 3662: loss=56.9927, lr=2.658e-05
step 3687: loss=69.7076, lr=2.430e-05
train_loss=68.3460  val_loss=34.7658
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 45/50
step 3721: loss=53.3171, lr=2.135e-05
step 3746: loss=54.9034, lr=1.931e-05
step 3771: loss=52.0784, lr=1.737e-05
train_loss=57.3431  val_loss=41.8461

Epoch 46/50
step 3805: loss=58.2274, lr=1.490e-05
step 3830: loss=114.8401, lr=1.321e-05
step 3855: loss=98.5072, lr=1.163e-05
train_loss=95.6507  val_loss=32.6905
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/best.pt

Epoch 47/50
step 3889: loss=70.9754, lr=9.648e-06
step 3914: loss=77.5168, lr=8.320e-06
step 3939: loss=69.9104, lr=7.102e-06
train_loss=67.8561  val_loss=33.8277

Epoch 48/50
step 3973: loss=51.1069, lr=5.620e-06
step 3998: loss=52.5838, lr=4.661e-06
step 4023: loss=52.9102, lr=3.812e-06
train_loss=52.3580  val_loss=39.2613

Epoch 49/50
step 4057: loss=51.4260, lr=2.837e-06
step 4082: loss=53.5525, lr=2.251e-06
step 4107: loss=59.0933, lr=1.777e-06
train_loss=58.4896  val_loss=33.9015

Epoch 50/50
step 4141: loss=51.5879, lr=1.313e-06
step 4166: loss=55.1538, lr=1.104e-06
step 4191: loss=56.1150, lr=1.007e-06
train_loss=56.0021  val_loss=32.8189

Test loss: 38.1721
Saved final checkpoint to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_013_lr0.0005_hd192_wd1.0_wr0.2/results/metrics_20251124T011433Z.json
