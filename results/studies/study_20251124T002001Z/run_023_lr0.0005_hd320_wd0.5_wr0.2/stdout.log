Using device: cuda
Dataset sizes â†’ train: 21361, val: 2670, test: 2670
Target scale (applied to F_box/F_true/residual): 1.461e+33

Epoch 1/50
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
step 25: loss=6695.7566, lr=2.695e-05
step 50: loss=6105.5928, lr=5.190e-05
step 75: loss=4832.8701, lr=7.685e-05
train_loss=4627.0690  val_loss=783.2241
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 2/50
step 109: loss=1837.7081, lr=1.108e-04
step 134: loss=1770.4498, lr=1.357e-04
step 159: loss=1445.6056, lr=1.607e-04
train_loss=1373.5020  val_loss=535.3779
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 3/50
step 193: loss=615.8086, lr=1.946e-04
step 218: loss=668.2142, lr=2.196e-04
step 243: loss=802.0568, lr=2.445e-04
train_loss=834.8227  val_loss=604.3248

Epoch 4/50
step 277: loss=846.1980, lr=2.784e-04
step 302: loss=1082.8089, lr=3.034e-04
step 327: loss=903.0971, lr=3.283e-04
train_loss=853.1137  val_loss=368.2306
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 5/50
step 361: loss=651.8434, lr=3.623e-04
step 386: loss=576.2726, lr=3.872e-04
step 411: loss=592.2580, lr=4.122e-04
train_loss=561.2777  val_loss=264.4405
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 6/50
step 445: loss=1120.0994, lr=4.461e-04
step 470: loss=909.7539, lr=4.711e-04
step 495: loss=756.0649, lr=4.960e-04
train_loss=713.5737  val_loss=192.8669
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 7/50
step 529: loss=659.0916, lr=4.999e-04
step 554: loss=482.2326, lr=4.997e-04
step 579: loss=433.9989, lr=4.994e-04
train_loss=440.8927  val_loss=249.4451

Epoch 8/50
step 613: loss=256.5828, lr=4.989e-04
step 638: loss=244.8387, lr=4.983e-04
step 663: loss=257.4737, lr=4.976e-04
train_loss=260.0319  val_loss=198.4103

Epoch 9/50
step 697: loss=274.0613, lr=4.965e-04
step 722: loss=247.6382, lr=4.956e-04
step 747: loss=236.9800, lr=4.945e-04
train_loss=274.2593  val_loss=149.6658
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 10/50
step 781: loss=503.8510, lr=4.929e-04
step 806: loss=407.2012, lr=4.916e-04
step 831: loss=348.6384, lr=4.902e-04
train_loss=329.9426  val_loss=114.1923
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 11/50
step 865: loss=207.1857, lr=4.881e-04
step 890: loss=207.7482, lr=4.864e-04
step 915: loss=210.5892, lr=4.847e-04
train_loss=206.5077  val_loss=147.2813

Epoch 12/50
step 949: loss=185.7031, lr=4.821e-04
step 974: loss=184.1780, lr=4.801e-04
step 999: loss=198.1213, lr=4.779e-04
train_loss=199.4975  val_loss=157.3369

Epoch 13/50
step 1033: loss=216.3537, lr=4.749e-04
step 1058: loss=183.3804, lr=4.725e-04
step 1083: loss=196.8906, lr=4.701e-04
train_loss=193.1549  val_loss=140.4159

Epoch 14/50
step 1117: loss=183.2662, lr=4.665e-04
step 1142: loss=159.8366, lr=4.638e-04
step 1167: loss=181.3971, lr=4.610e-04
train_loss=175.7862  val_loss=98.5164
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 15/50
step 1201: loss=122.6623, lr=4.571e-04
step 1226: loss=136.8760, lr=4.541e-04
step 1251: loss=139.0906, lr=4.510e-04
train_loss=141.1776  val_loss=157.6701

Epoch 16/50
step 1285: loss=128.7551, lr=4.466e-04
step 1310: loss=127.7468, lr=4.433e-04
step 1335: loss=117.9820, lr=4.399e-04
train_loss=118.0257  val_loss=119.4155

Epoch 17/50
step 1369: loss=126.4000, lr=4.351e-04
step 1394: loss=123.4130, lr=4.315e-04
step 1419: loss=131.8279, lr=4.278e-04
train_loss=132.8180  val_loss=108.7181

Epoch 18/50
step 1453: loss=101.8499, lr=4.227e-04
step 1478: loss=127.0280, lr=4.188e-04
step 1503: loss=119.3692, lr=4.149e-04
train_loss=123.4254  val_loss=198.2555

Epoch 19/50
step 1537: loss=123.3323, lr=4.094e-04
step 1562: loss=118.5536, lr=4.053e-04
step 1587: loss=127.1374, lr=4.011e-04
train_loss=122.5657  val_loss=51.0474
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 20/50
step 1621: loss=97.9646, lr=3.953e-04
step 1646: loss=102.4737, lr=3.909e-04
step 1671: loss=97.3919, lr=3.865e-04
train_loss=98.0302  val_loss=121.5312

Epoch 21/50
step 1705: loss=110.1549, lr=3.804e-04
step 1730: loss=111.8945, lr=3.759e-04
step 1755: loss=107.8224, lr=3.713e-04
train_loss=112.5421  val_loss=80.0447

Epoch 22/50
step 1789: loss=155.3800, lr=3.649e-04
step 1814: loss=129.2684, lr=3.602e-04
step 1839: loss=113.7958, lr=3.554e-04
train_loss=113.0681  val_loss=67.7799

Epoch 23/50
step 1873: loss=141.7976, lr=3.488e-04
step 1898: loss=122.3496, lr=3.439e-04
step 1923: loss=109.3266, lr=3.390e-04
train_loss=107.2730  val_loss=55.9672

Epoch 24/50
step 1957: loss=87.3135, lr=3.322e-04
step 1982: loss=94.0568, lr=3.272e-04
step 2007: loss=92.9874, lr=3.221e-04
train_loss=91.6596  val_loss=69.4145

Epoch 25/50
step 2041: loss=134.3211, lr=3.152e-04
step 2066: loss=118.0792, lr=3.101e-04
step 2091: loss=109.8209, lr=3.049e-04
train_loss=108.2753  val_loss=68.7646

Epoch 26/50
step 2125: loss=96.6874, lr=2.979e-04
step 2150: loss=102.8084, lr=2.927e-04
step 2175: loss=92.5497, lr=2.874e-04
train_loss=91.3592  val_loss=60.2273

Epoch 27/50
step 2209: loss=89.4013, lr=2.803e-04
step 2234: loss=91.7747, lr=2.750e-04
step 2259: loss=85.8576, lr=2.698e-04
train_loss=85.1229  val_loss=57.3776

Epoch 28/50
step 2293: loss=91.8144, lr=2.626e-04
step 2318: loss=87.2366, lr=2.573e-04
step 2343: loss=84.8568, lr=2.520e-04
train_loss=83.7762  val_loss=45.3317
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 29/50
step 2377: loss=69.0558, lr=2.448e-04
step 2402: loss=82.3843, lr=2.395e-04
step 2427: loss=83.7176, lr=2.342e-04
train_loss=81.6592  val_loss=57.2049

Epoch 30/50
step 2461: loss=86.0597, lr=2.270e-04
step 2486: loss=85.6386, lr=2.218e-04
step 2511: loss=80.5889, lr=2.165e-04
train_loss=81.1355  val_loss=48.8792

Epoch 31/50
step 2545: loss=72.7549, lr=2.094e-04
step 2570: loss=93.7046, lr=2.042e-04
step 2595: loss=88.9787, lr=1.990e-04
train_loss=86.0690  val_loss=45.4531

Epoch 32/50
step 2629: loss=64.5547, lr=1.919e-04
step 2654: loss=66.2108, lr=1.868e-04
step 2679: loss=75.0352, lr=1.817e-04
train_loss=77.5239  val_loss=54.0745

Epoch 33/50
step 2713: loss=72.2302, lr=1.748e-04
step 2738: loss=73.8601, lr=1.698e-04
step 2763: loss=75.0065, lr=1.648e-04
train_loss=77.6737  val_loss=73.2980

Epoch 34/50
step 2797: loss=77.5003, lr=1.581e-04
step 2822: loss=80.6374, lr=1.532e-04
step 2847: loss=79.7386, lr=1.483e-04
train_loss=78.5897  val_loss=58.8149

Epoch 35/50
step 2881: loss=64.8662, lr=1.418e-04
step 2906: loss=69.4193, lr=1.370e-04
step 2931: loss=68.5140, lr=1.323e-04
train_loss=67.5226  val_loss=48.0071

Epoch 36/50
step 2965: loss=59.9279, lr=1.261e-04
step 2990: loss=61.4608, lr=1.215e-04
step 3015: loss=1085.6751, lr=1.170e-04
train_loss=982.2240  val_loss=39.4308
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 37/50
step 3049: loss=91.8419, lr=1.110e-04
step 3074: loss=78.1267, lr=1.066e-04
step 3099: loss=77.7721, lr=1.023e-04
train_loss=75.4721  val_loss=46.3822

Epoch 38/50
step 3133: loss=56.8491, lr=9.658e-05
step 3158: loss=57.8960, lr=9.244e-05
step 3183: loss=58.4568, lr=8.838e-05
train_loss=57.5095  val_loss=39.1759
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 39/50
step 3217: loss=62.7255, lr=8.298e-05
step 3242: loss=58.5473, lr=7.909e-05
step 3267: loss=56.9992, lr=7.528e-05
train_loss=57.8983  val_loss=41.4338

Epoch 40/50
step 3301: loss=55.1262, lr=7.023e-05
step 3326: loss=60.3567, lr=6.660e-05
step 3351: loss=58.0143, lr=6.307e-05
train_loss=57.0789  val_loss=35.4905
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 41/50
step 3385: loss=53.4151, lr=5.839e-05
step 3410: loss=55.4155, lr=5.506e-05
step 3435: loss=71.2355, lr=5.181e-05
train_loss=68.8865  val_loss=124.8175

Epoch 42/50
step 3469: loss=50.9135, lr=4.754e-05
step 3494: loss=54.0817, lr=4.450e-05
step 3519: loss=57.4623, lr=4.156e-05
train_loss=56.1789  val_loss=32.9034
Saved improved checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/best.pt

Epoch 43/50
step 3553: loss=64.5840, lr=3.771e-05
step 3578: loss=65.5626, lr=3.499e-05
step 3603: loss=63.0794, lr=3.237e-05
train_loss=63.0944  val_loss=33.3629

Epoch 44/50
step 3637: loss=62.5990, lr=2.897e-05
step 3662: loss=57.9067, lr=2.658e-05
step 3687: loss=55.9513, lr=2.430e-05
train_loss=54.9533  val_loss=35.3632

Epoch 45/50
step 3721: loss=77.3179, lr=2.135e-05
step 3746: loss=67.1619, lr=1.931e-05
step 3771: loss=61.0198, lr=1.737e-05
train_loss=61.5348  val_loss=34.7480

Epoch 46/50
step 3805: loss=92.1775, lr=1.490e-05
step 3830: loss=76.2445, lr=1.321e-05
step 3855: loss=68.3749, lr=1.163e-05
train_loss=67.0720  val_loss=44.1234

Epoch 47/50
step 3889: loss=53.5849, lr=9.648e-06
step 3914: loss=55.4397, lr=8.320e-06
step 3939: loss=54.3508, lr=7.102e-06
train_loss=53.8116  val_loss=33.8708

Epoch 48/50
step 3973: loss=48.9848, lr=5.620e-06
step 3998: loss=48.2614, lr=4.661e-06
step 4023: loss=52.7497, lr=3.812e-06
train_loss=54.1630  val_loss=35.9144

Epoch 49/50
step 4057: loss=70.6962, lr=2.837e-06
step 4082: loss=61.1863, lr=2.251e-06
step 4107: loss=58.1903, lr=1.777e-06
train_loss=60.1320  val_loss=33.1625

Epoch 50/50
step 4141: loss=51.2881, lr=1.313e-06
step 4166: loss=67.2123, lr=1.104e-06
step 4191: loss=60.9100, lr=1.007e-06
train_loss=60.3695  val_loss=35.7114

Test loss: 40.5950
Saved final checkpoint to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/checkpoints/final.pt
Saved metrics to results/studies/study_20251124T002001Z/run_023_lr0.0005_hd320_wd0.5_wr0.2/results/metrics_20251124T015434Z.json
